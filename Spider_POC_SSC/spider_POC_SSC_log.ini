2023-03-28 13:37:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:37:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:37:48 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:37:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:37:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:37:48 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:37:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:37:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:37:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:37:48 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:37:48 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:37:48 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:37:48 [scrapy.extensions.telnet] INFO: Telnet Password: 82686d79c08e57de
2023-03-28 13:37:48 [scrapy.extensions.telnet] INFO: Telnet Password: 396ac7edc71e5fd6
2023-03-28 13:37:48 [scrapy.extensions.telnet] INFO: Telnet Password: a9a47037fe44de95
2023-03-28 13:37:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:37:48 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:37:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:37:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:37:48 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:37:48 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:37:48 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:37:48 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:37:48 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:37:49 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:37:49 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:37:49 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:37:52 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:37:52 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:37:52 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:37:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:37:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:37:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:37:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:37:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:37:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:37:53 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:37:53 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:37:53 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:37:53 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:37:53 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:37:53 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:37:54 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:37:54 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:37:54 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:37:59 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:37:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:37:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-28 13:38:00 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:38:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:38:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-28 13:38:00 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:38:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:38:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-28 13:38:28 [root] INFO: First page:2527, Max page:3790
2023-03-28 13:38:33 [root] INFO: First page:1264, Max page:2526
2023-03-28 13:38:41 [root] INFO: First page:1, Max page:1263
2023-03-28 13:39:01 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 13:39:35 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:39:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 121, in get_company_name_MDN
    self.wait.until(EC.visibility_of_element_located((By.ID, 'pt2:t2:0:c3')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x000000010bc14428 chromedriver + 4899880
1   chromedriver                        0x000000010bb91a23 chromedriver + 4364835
2   chromedriver                        0x000000010b7dbbf6 chromedriver + 474102
3   chromedriver                        0x000000010b81f4f0 chromedriver + 750832
4   chromedriver                        0x000000010b81f751 chromedriver + 751441
5   chromedriver                        0x000000010b863834 chromedriver + 1030196
6   chromedriver                        0x000000010b84558d chromedriver + 906637
7   chromedriver                        0x000000010b860b5b chromedriver + 1018715
8   chromedriver                        0x000000010b845333 chromedriver + 906035
9   chromedriver                        0x000000010b80f55f chromedriver + 685407
10  chromedriver                        0x000000010b810a7e chromedriver + 690814
11  chromedriver                        0x000000010bbe179e chromedriver + 4691870
12  chromedriver                        0x000000010bbe6961 chromedriver + 4712801
13  chromedriver                        0x000000010bbed2ff chromedriver + 4739839
14  chromedriver                        0x000000010bbe785a chromedriver + 4716634
15  chromedriver                        0x000000010bbb9fce chromedriver + 4530126
16  chromedriver                        0x000000010bc075c8 chromedriver + 4847048
17  chromedriver                        0x000000010bc07747 chromedriver + 4847431
18  chromedriver                        0x000000010bc1c87f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 78, in get_company_finance_data
    company_name, company_MDN = self.get_company_name_MDN()
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 123, in get_company_name_MDN
    if self.driver.find_element(By.ID, 'pt2:t2::emptyTxt'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="pt2:t2::emptyTxt"]"}
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010bc14428 chromedriver + 4899880
1   chromedriver                        0x000000010bb91a23 chromedriver + 4364835
2   chromedriver                        0x000000010b7dbbf6 chromedriver + 474102
3   chromedriver                        0x000000010b81f4f0 chromedriver + 750832
4   chromedriver                        0x000000010b81f751 chromedriver + 751441
5   chromedriver                        0x000000010b863834 chromedriver + 1030196
6   chromedriver                        0x000000010b84558d chromedriver + 906637
7   chromedriver                        0x000000010b860b5b chromedriver + 1018715
8   chromedriver                        0x000000010b845333 chromedriver + 906035
9   chromedriver                        0x000000010b80f55f chromedriver + 685407
10  chromedriver                        0x000000010b810a7e chromedriver + 690814
11  chromedriver                        0x000000010bbe179e chromedriver + 4691870
12  chromedriver                        0x000000010bbe6961 chromedriver + 4712801
13  chromedriver                        0x000000010bbed2ff chromedriver + 4739839
14  chromedriver                        0x000000010bbe785a chromedriver + 4716634
15  chromedriver                        0x000000010bbb9fce chromedriver + 4530126
16  chromedriver                        0x000000010bc075c8 chromedriver + 4847048
17  chromedriver                        0x000000010bc07747 chromedriver + 4847431
18  chromedriver                        0x000000010bc1c87f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 91, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
0   chromedriver                        0x000000010bc14428 chromedriver + 4899880
1   chromedriver                        0x000000010bb91a23 chromedriver + 4364835
2   chromedriver                        0x000000010b7dba71 chromedriver + 473713
3   chromedriver                        0x000000010b80f105 chromedriver + 684293
4   chromedriver                        0x000000010b810a7e chromedriver + 690814
5   chromedriver                        0x000000010bbe179e chromedriver + 4691870
6   chromedriver                        0x000000010bbe6961 chromedriver + 4712801
7   chromedriver                        0x000000010bbed2ff chromedriver + 4739839
8   chromedriver                        0x000000010bbe785a chromedriver + 4716634
9   chromedriver                        0x000000010bbb9fce chromedriver + 4530126
10  chromedriver                        0x000000010bc075c8 chromedriver + 4847048
11  chromedriver                        0x000000010bc07747 chromedriver + 4847431
12  chromedriver                        0x000000010bc1c87f chromedriver + 4933759
13  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
14  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:39:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:39:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:39:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5672,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 95.539062,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 39, 35, 598867),
 'httpcompression/response_bytes': 11703,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 132829184,
 'memusage/startup': 128614400,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSessionIdException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 38, 0, 59805)}
2023-03-28 13:39:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:39:40 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:39:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 121, in get_company_name_MDN
    self.wait.until(EC.visibility_of_element_located((By.ID, 'pt2:t2:0:c3')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x00000001067ad428 chromedriver + 4899880
1   chromedriver                        0x000000010672aa23 chromedriver + 4364835
2   chromedriver                        0x0000000106374bf6 chromedriver + 474102
3   chromedriver                        0x00000001063b84f0 chromedriver + 750832
4   chromedriver                        0x00000001063b8751 chromedriver + 751441
5   chromedriver                        0x00000001063fc834 chromedriver + 1030196
6   chromedriver                        0x00000001063de58d chromedriver + 906637
7   chromedriver                        0x00000001063f9b5b chromedriver + 1018715
8   chromedriver                        0x00000001063de333 chromedriver + 906035
9   chromedriver                        0x00000001063a855f chromedriver + 685407
10  chromedriver                        0x00000001063a9a7e chromedriver + 690814
11  chromedriver                        0x000000010677a79e chromedriver + 4691870
12  chromedriver                        0x000000010677f961 chromedriver + 4712801
13  chromedriver                        0x00000001067862ff chromedriver + 4739839
14  chromedriver                        0x000000010678085a chromedriver + 4716634
15  chromedriver                        0x0000000106752fce chromedriver + 4530126
16  chromedriver                        0x00000001067a05c8 chromedriver + 4847048
17  chromedriver                        0x00000001067a0747 chromedriver + 4847431
18  chromedriver                        0x00000001067b587f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 78, in get_company_finance_data
    company_name, company_MDN = self.get_company_name_MDN()
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 123, in get_company_name_MDN
    if self.driver.find_element(By.ID, 'pt2:t2::emptyTxt'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="pt2:t2::emptyTxt"]"}
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x00000001067ad428 chromedriver + 4899880
1   chromedriver                        0x000000010672aa23 chromedriver + 4364835
2   chromedriver                        0x0000000106374bf6 chromedriver + 474102
3   chromedriver                        0x00000001063b84f0 chromedriver + 750832
4   chromedriver                        0x00000001063b8751 chromedriver + 751441
5   chromedriver                        0x00000001063fc834 chromedriver + 1030196
6   chromedriver                        0x00000001063de58d chromedriver + 906637
7   chromedriver                        0x00000001063f9b5b chromedriver + 1018715
8   chromedriver                        0x00000001063de333 chromedriver + 906035
9   chromedriver                        0x00000001063a855f chromedriver + 685407
10  chromedriver                        0x00000001063a9a7e chromedriver + 690814
11  chromedriver                        0x000000010677a79e chromedriver + 4691870
12  chromedriver                        0x000000010677f961 chromedriver + 4712801
13  chromedriver                        0x00000001067862ff chromedriver + 4739839
14  chromedriver                        0x000000010678085a chromedriver + 4716634
15  chromedriver                        0x0000000106752fce chromedriver + 4530126
16  chromedriver                        0x00000001067a05c8 chromedriver + 4847048
17  chromedriver                        0x00000001067a0747 chromedriver + 4847431
18  chromedriver                        0x00000001067b587f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 91, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
0   chromedriver                        0x00000001067ad428 chromedriver + 4899880
1   chromedriver                        0x000000010672aa23 chromedriver + 4364835
2   chromedriver                        0x0000000106374a71 chromedriver + 473713
3   chromedriver                        0x00000001063a8105 chromedriver + 684293
4   chromedriver                        0x00000001063a9a7e chromedriver + 690814
5   chromedriver                        0x000000010677a79e chromedriver + 4691870
6   chromedriver                        0x000000010677f961 chromedriver + 4712801
7   chromedriver                        0x00000001067862ff chromedriver + 4739839
8   chromedriver                        0x000000010678085a chromedriver + 4716634
9   chromedriver                        0x0000000106752fce chromedriver + 4530126
10  chromedriver                        0x00000001067a05c8 chromedriver + 4847048
11  chromedriver                        0x00000001067a0747 chromedriver + 4847431
12  chromedriver                        0x00000001067b587f chromedriver + 4933759
13  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
14  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:39:40 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:39:40 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:39:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 680,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 80964,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 100.369419,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 39, 40, 415183),
 'httpcompression/response_bytes': 259365,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 133251072,
 'memusage/startup': 128651264,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSessionIdException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 38, 0, 45764)}
2023-03-28 13:39:40 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:40:35 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:40:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 84, in get_company_finance_data
    self.get_report_data(year=year, quarter=quarter, report_name = report_name, company_name=company_name, company_MDN=company_MDN)
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 156, in get_report_data
    economic_infor_report_data = self.wait.until(EC.visibility_of_element_located((By.ID, f'pt2:tab{tab_num}::body'))).get_attribute('innerHTML')
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 139, in _predicate
    return _element_if_visible(driver.find_element(*locator))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 162, in _element_if_visible
    return element if element.is_displayed() == visibility else False
                      ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'is_displayed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 90, in get_company_finance_data
    self.driver.close()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 551, in close
    self.execute(Command.CLOSE)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010427a428 chromedriver + 4899880
1   chromedriver                        0x00000001041f7a23 chromedriver + 4364835
2   chromedriver                        0x0000000103e41bf6 chromedriver + 474102
3   chromedriver                        0x0000000103e17cdc chromedriver + 302300
4   chromedriver                        0x0000000103eb085f chromedriver + 927839
5   chromedriver                        0x0000000103eb75ac chromedriver + 955820
6   chromedriver                        0x0000000103eab528 chromedriver + 906536
7   chromedriver                        0x0000000103e7555f chromedriver + 685407
8   chromedriver                        0x0000000103e76a7e chromedriver + 690814
9   chromedriver                        0x000000010424779e chromedriver + 4691870
10  chromedriver                        0x000000010424c961 chromedriver + 4712801
11  chromedriver                        0x00000001042532ff chromedriver + 4739839
12  chromedriver                        0x000000010424d85a chromedriver + 4716634
13  chromedriver                        0x000000010421ffce chromedriver + 4530126
14  chromedriver                        0x000000010426d5c8 chromedriver + 4847048
15  chromedriver                        0x000000010426d747 chromedriver + 4847431
16  chromedriver                        0x000000010428287f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:40:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2023-03-28 13:40:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:40:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 584,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 8254,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 155.758909,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 40, 35, 511602),
 'httpcompression/response_bytes': 7240,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 18,
 'log_count/WARNING': 3,
 'memusage/max': 140267520,
 'memusage/startup': 128184320,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 37, 59, 752693)}
2023-03-28 13:40:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:42:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:42:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:42:20 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:42:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:42:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:42:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:42:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:42:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:42:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:42:20 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:42:20 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:42:20 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:42:20 [scrapy.extensions.telnet] INFO: Telnet Password: 53a144cb6b2ae036
2023-03-28 13:42:20 [scrapy.extensions.telnet] INFO: Telnet Password: f78a67f22a6cc0dd
2023-03-28 13:42:20 [scrapy.extensions.telnet] INFO: Telnet Password: b38ee212221b4361
2023-03-28 13:42:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:42:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:42:20 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:42:20 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:42:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:42:20 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:42:20 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:42:20 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:42:20 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:42:21 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:42:21 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:42:21 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:42:24 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:42:24 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:42:24 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:42:24 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:42:24 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:42:24 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:42:24 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:42:25 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:42:26 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:42:26 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:42:31 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:42:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:42:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-28 13:42:31 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:42:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:42:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-28 13:42:31 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:42:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:42:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-28 13:43:00 [root] INFO: First page:1, Max page:1263
2023-03-28 13:43:04 [root] INFO: First page:2527, Max page:3791
2023-03-28 13:43:08 [root] INFO: First page:1264, Max page:2526
2023-03-28 13:43:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 2 items (at 2 items/min)
2023-03-28 13:44:11 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:44:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 121, in get_company_name_MDN
    self.wait.until(EC.visibility_of_element_located((By.ID, 'pt2:t2:0:c3')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x000000010c47a428 chromedriver + 4899880
1   chromedriver                        0x000000010c3f7a23 chromedriver + 4364835
2   chromedriver                        0x000000010c041bf6 chromedriver + 474102
3   chromedriver                        0x000000010c0854f0 chromedriver + 750832
4   chromedriver                        0x000000010c085751 chromedriver + 751441
5   chromedriver                        0x000000010c0c9834 chromedriver + 1030196
6   chromedriver                        0x000000010c0ab58d chromedriver + 906637
7   chromedriver                        0x000000010c0c6b5b chromedriver + 1018715
8   chromedriver                        0x000000010c0ab333 chromedriver + 906035
9   chromedriver                        0x000000010c07555f chromedriver + 685407
10  chromedriver                        0x000000010c076a7e chromedriver + 690814
11  chromedriver                        0x000000010c44779e chromedriver + 4691870
12  chromedriver                        0x000000010c44c961 chromedriver + 4712801
13  chromedriver                        0x000000010c4532ff chromedriver + 4739839
14  chromedriver                        0x000000010c44d85a chromedriver + 4716634
15  chromedriver                        0x000000010c41ffce chromedriver + 4530126
16  chromedriver                        0x000000010c46d5c8 chromedriver + 4847048
17  chromedriver                        0x000000010c46d747 chromedriver + 4847431
18  chromedriver                        0x000000010c48287f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 78, in get_company_finance_data
    company_name, company_MDN = self.get_company_name_MDN()
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 123, in get_company_name_MDN
    if self.driver.find_element(By.ID, 'pt2:t2::emptyTxt'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="pt2:t2::emptyTxt"]"}
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010c47a428 chromedriver + 4899880
1   chromedriver                        0x000000010c3f7a23 chromedriver + 4364835
2   chromedriver                        0x000000010c041bf6 chromedriver + 474102
3   chromedriver                        0x000000010c0854f0 chromedriver + 750832
4   chromedriver                        0x000000010c085751 chromedriver + 751441
5   chromedriver                        0x000000010c0c9834 chromedriver + 1030196
6   chromedriver                        0x000000010c0ab58d chromedriver + 906637
7   chromedriver                        0x000000010c0c6b5b chromedriver + 1018715
8   chromedriver                        0x000000010c0ab333 chromedriver + 906035
9   chromedriver                        0x000000010c07555f chromedriver + 685407
10  chromedriver                        0x000000010c076a7e chromedriver + 690814
11  chromedriver                        0x000000010c44779e chromedriver + 4691870
12  chromedriver                        0x000000010c44c961 chromedriver + 4712801
13  chromedriver                        0x000000010c4532ff chromedriver + 4739839
14  chromedriver                        0x000000010c44d85a chromedriver + 4716634
15  chromedriver                        0x000000010c41ffce chromedriver + 4530126
16  chromedriver                        0x000000010c46d5c8 chromedriver + 4847048
17  chromedriver                        0x000000010c46d747 chromedriver + 4847431
18  chromedriver                        0x000000010c48287f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 91, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
0   chromedriver                        0x000000010c47a428 chromedriver + 4899880
1   chromedriver                        0x000000010c3f7a23 chromedriver + 4364835
2   chromedriver                        0x000000010c041a71 chromedriver + 473713
3   chromedriver                        0x000000010c075105 chromedriver + 684293
4   chromedriver                        0x000000010c076a7e chromedriver + 690814
5   chromedriver                        0x000000010c44779e chromedriver + 4691870
6   chromedriver                        0x000000010c44c961 chromedriver + 4712801
7   chromedriver                        0x000000010c4532ff chromedriver + 4739839
8   chromedriver                        0x000000010c44d85a chromedriver + 4716634
9   chromedriver                        0x000000010c41ffce chromedriver + 4530126
10  chromedriver                        0x000000010c46d5c8 chromedriver + 4847048
11  chromedriver                        0x000000010c46d747 chromedriver + 4847431
12  chromedriver                        0x000000010c48287f chromedriver + 4933759
13  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
14  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:44:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:44:11 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:44:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 656,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 80640,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 100.06929,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 44, 11, 662360),
 'httpcompression/response_bytes': 257179,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 133873664,
 'memusage/startup': 129351680,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSessionIdException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 42, 31, 593070)}
2023-03-28 13:44:11 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:44:15 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:44:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 121, in get_company_name_MDN
    self.wait.until(EC.visibility_of_element_located((By.ID, 'pt2:t2:0:c3')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x000000010e028428 chromedriver + 4899880
1   chromedriver                        0x000000010dfa5a23 chromedriver + 4364835
2   chromedriver                        0x000000010dbefbf6 chromedriver + 474102
3   chromedriver                        0x000000010dc334f0 chromedriver + 750832
4   chromedriver                        0x000000010dc33751 chromedriver + 751441
5   chromedriver                        0x000000010dc77834 chromedriver + 1030196
6   chromedriver                        0x000000010dc5958d chromedriver + 906637
7   chromedriver                        0x000000010dc74b5b chromedriver + 1018715
8   chromedriver                        0x000000010dc59333 chromedriver + 906035
9   chromedriver                        0x000000010dc2355f chromedriver + 685407
10  chromedriver                        0x000000010dc24a7e chromedriver + 690814
11  chromedriver                        0x000000010dff579e chromedriver + 4691870
12  chromedriver                        0x000000010dffa961 chromedriver + 4712801
13  chromedriver                        0x000000010e0012ff chromedriver + 4739839
14  chromedriver                        0x000000010dffb85a chromedriver + 4716634
15  chromedriver                        0x000000010dfcdfce chromedriver + 4530126
16  chromedriver                        0x000000010e01b5c8 chromedriver + 4847048
17  chromedriver                        0x000000010e01b747 chromedriver + 4847431
18  chromedriver                        0x000000010e03087f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 78, in get_company_finance_data
    company_name, company_MDN = self.get_company_name_MDN()
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 123, in get_company_name_MDN
    if self.driver.find_element(By.ID, 'pt2:t2::emptyTxt'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="pt2:t2::emptyTxt"]"}
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010e028428 chromedriver + 4899880
1   chromedriver                        0x000000010dfa5a23 chromedriver + 4364835
2   chromedriver                        0x000000010dbefbf6 chromedriver + 474102
3   chromedriver                        0x000000010dc334f0 chromedriver + 750832
4   chromedriver                        0x000000010dc33751 chromedriver + 751441
5   chromedriver                        0x000000010dc77834 chromedriver + 1030196
6   chromedriver                        0x000000010dc5958d chromedriver + 906637
7   chromedriver                        0x000000010dc74b5b chromedriver + 1018715
8   chromedriver                        0x000000010dc59333 chromedriver + 906035
9   chromedriver                        0x000000010dc2355f chromedriver + 685407
10  chromedriver                        0x000000010dc24a7e chromedriver + 690814
11  chromedriver                        0x000000010dff579e chromedriver + 4691870
12  chromedriver                        0x000000010dffa961 chromedriver + 4712801
13  chromedriver                        0x000000010e0012ff chromedriver + 4739839
14  chromedriver                        0x000000010dffb85a chromedriver + 4716634
15  chromedriver                        0x000000010dfcdfce chromedriver + 4530126
16  chromedriver                        0x000000010e01b5c8 chromedriver + 4847048
17  chromedriver                        0x000000010e01b747 chromedriver + 4847431
18  chromedriver                        0x000000010e03087f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 91, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
0   chromedriver                        0x000000010e028428 chromedriver + 4899880
1   chromedriver                        0x000000010dfa5a23 chromedriver + 4364835
2   chromedriver                        0x000000010dbefa71 chromedriver + 473713
3   chromedriver                        0x000000010dc23105 chromedriver + 684293
4   chromedriver                        0x000000010dc24a7e chromedriver + 690814
5   chromedriver                        0x000000010dff579e chromedriver + 4691870
6   chromedriver                        0x000000010dffa961 chromedriver + 4712801
7   chromedriver                        0x000000010e0012ff chromedriver + 4739839
8   chromedriver                        0x000000010dffb85a chromedriver + 4716634
9   chromedriver                        0x000000010dfcdfce chromedriver + 4530126
10  chromedriver                        0x000000010e01b5c8 chromedriver + 4847048
11  chromedriver                        0x000000010e01b747 chromedriver + 4847431
12  chromedriver                        0x000000010e03087f chromedriver + 4933759
13  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
14  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:44:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:44:15 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:44:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 648,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 85402,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 104.175267,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 44, 15, 716172),
 'httpcompression/response_bytes': 269070,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 134336512,
 'memusage/startup': 129155072,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSessionIdException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 42, 31, 540905)}
2023-03-28 13:44:15 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:44:27 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:44:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 70, in get_company_finance_data
    self.wait_until_done()
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 103, in wait_until_done
    self.wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 69, in _predicate
    return driver.find_element(*locator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000108453428 chromedriver + 4899880
1   chromedriver                        0x00000001083d0a23 chromedriver + 4364835
2   chromedriver                        0x000000010801abf6 chromedriver + 474102
3   chromedriver                        0x0000000107ff0cdc chromedriver + 302300
4   chromedriver                        0x000000010808985f chromedriver + 927839
5   chromedriver                        0x000000010809f226 chromedriver + 1016358
6   chromedriver                        0x0000000108084333 chromedriver + 906035
7   chromedriver                        0x000000010804e55f chromedriver + 685407
8   chromedriver                        0x000000010804fa7e chromedriver + 690814
9   chromedriver                        0x000000010842079e chromedriver + 4691870
10  chromedriver                        0x0000000108425961 chromedriver + 4712801
11  chromedriver                        0x000000010842c2ff chromedriver + 4739839
12  chromedriver                        0x000000010842685a chromedriver + 4716634
13  chromedriver                        0x00000001083f8fce chromedriver + 4530126
14  chromedriver                        0x00000001084465c8 chromedriver + 4847048
15  chromedriver                        0x0000000108446747 chromedriver + 4847431
16  chromedriver                        0x000000010845b87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 90, in get_company_finance_data
    self.driver.close()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 551, in close
    self.execute(Command.CLOSE)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000108453428 chromedriver + 4899880
1   chromedriver                        0x00000001083d0a23 chromedriver + 4364835
2   chromedriver                        0x000000010801abf6 chromedriver + 474102
3   chromedriver                        0x0000000107ff0cdc chromedriver + 302300
4   chromedriver                        0x000000010808985f chromedriver + 927839
5   chromedriver                        0x00000001080905ac chromedriver + 955820
6   chromedriver                        0x0000000108084528 chromedriver + 906536
7   chromedriver                        0x000000010804e55f chromedriver + 685407
8   chromedriver                        0x000000010804fa7e chromedriver + 690814
9   chromedriver                        0x000000010842079e chromedriver + 4691870
10  chromedriver                        0x0000000108425961 chromedriver + 4712801
11  chromedriver                        0x000000010842c2ff chromedriver + 4739839
12  chromedriver                        0x000000010842685a chromedriver + 4716634
13  chromedriver                        0x00000001083f8fce chromedriver + 4530126
14  chromedriver                        0x00000001084465c8 chromedriver + 4847048
15  chromedriver                        0x0000000108446747 chromedriver + 4847431
16  chromedriver                        0x000000010845b87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:44:27 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:44:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 629,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 4787,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 116.274336,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 44, 27, 626496),
 'httpcompression/response_bytes': 9156,
 'httpcompression/response_count': 2,
 'item_scraped_count': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 142229504,
 'memusage/startup': 128950272,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 42, 31, 352160)}
2023-03-28 13:44:27 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:45:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:45:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:45:12 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:45:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:45:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:45:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:45:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:45:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:45:12 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:45:12 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:45:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:45:12 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:45:12 [scrapy.extensions.telnet] INFO: Telnet Password: 5efa9ecbf772e473
2023-03-28 13:45:12 [scrapy.extensions.telnet] INFO: Telnet Password: 6abc5733ac9ccf42
2023-03-28 13:45:12 [scrapy.extensions.telnet] INFO: Telnet Password: cb0771c9b375e7f0
2023-03-28 13:45:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:45:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:45:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:45:12 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:45:12 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:45:12 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:45:13 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:45:13 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:45:13 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:45:13 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:45:13 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:45:13 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:45:16 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:45:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:45:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:45:16 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:45:16 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:45:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:45:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:45:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:45:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:45:17 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:45:17 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:45:17 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:45:17 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:45:17 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:45:17 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:45:18 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:45:18 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:45:18 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:45:23 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:45:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:45:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-28 13:45:23 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:45:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:45:23 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:45:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-28 13:45:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:45:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-28 13:45:59 [root] INFO: First page:1, Max page:1263
2023-03-28 13:46:04 [root] INFO: First page:2527, Max page:3791
2023-03-28 13:46:05 [root] INFO: First page:1264, Max page:2526
2023-03-28 13:46:32 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 13:47:10 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:47:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 121, in get_company_name_MDN
    self.wait.until(EC.visibility_of_element_located((By.ID, 'pt2:t2:0:c3')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x00000001006bc428 chromedriver + 4899880
1   chromedriver                        0x0000000100639a23 chromedriver + 4364835
2   chromedriver                        0x0000000100283bf6 chromedriver + 474102
3   chromedriver                        0x00000001002c74f0 chromedriver + 750832
4   chromedriver                        0x00000001002c7751 chromedriver + 751441
5   chromedriver                        0x000000010030b834 chromedriver + 1030196
6   chromedriver                        0x00000001002ed58d chromedriver + 906637
7   chromedriver                        0x0000000100308b5b chromedriver + 1018715
8   chromedriver                        0x00000001002ed333 chromedriver + 906035
9   chromedriver                        0x00000001002b755f chromedriver + 685407
10  chromedriver                        0x00000001002b8a7e chromedriver + 690814
11  chromedriver                        0x000000010068979e chromedriver + 4691870
12  chromedriver                        0x000000010068e961 chromedriver + 4712801
13  chromedriver                        0x00000001006952ff chromedriver + 4739839
14  chromedriver                        0x000000010068f85a chromedriver + 4716634
15  chromedriver                        0x0000000100661fce chromedriver + 4530126
16  chromedriver                        0x00000001006af5c8 chromedriver + 4847048
17  chromedriver                        0x00000001006af747 chromedriver + 4847431
18  chromedriver                        0x00000001006c487f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 78, in get_company_finance_data
    company_name, company_MDN = self.get_company_name_MDN()
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 123, in get_company_name_MDN
    if self.driver.find_element(By.ID, 'pt2:t2::emptyTxt'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="pt2:t2::emptyTxt"]"}
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x00000001006bc428 chromedriver + 4899880
1   chromedriver                        0x0000000100639a23 chromedriver + 4364835
2   chromedriver                        0x0000000100283bf6 chromedriver + 474102
3   chromedriver                        0x00000001002c74f0 chromedriver + 750832
4   chromedriver                        0x00000001002c7751 chromedriver + 751441
5   chromedriver                        0x000000010030b834 chromedriver + 1030196
6   chromedriver                        0x00000001002ed58d chromedriver + 906637
7   chromedriver                        0x0000000100308b5b chromedriver + 1018715
8   chromedriver                        0x00000001002ed333 chromedriver + 906035
9   chromedriver                        0x00000001002b755f chromedriver + 685407
10  chromedriver                        0x00000001002b8a7e chromedriver + 690814
11  chromedriver                        0x000000010068979e chromedriver + 4691870
12  chromedriver                        0x000000010068e961 chromedriver + 4712801
13  chromedriver                        0x00000001006952ff chromedriver + 4739839
14  chromedriver                        0x000000010068f85a chromedriver + 4716634
15  chromedriver                        0x0000000100661fce chromedriver + 4530126
16  chromedriver                        0x00000001006af5c8 chromedriver + 4847048
17  chromedriver                        0x00000001006af747 chromedriver + 4847431
18  chromedriver                        0x00000001006c487f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 91, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
0   chromedriver                        0x00000001006bc428 chromedriver + 4899880
1   chromedriver                        0x0000000100639a23 chromedriver + 4364835
2   chromedriver                        0x0000000100283a71 chromedriver + 473713
3   chromedriver                        0x00000001002b7105 chromedriver + 684293
4   chromedriver                        0x00000001002b8a7e chromedriver + 690814
5   chromedriver                        0x000000010068979e chromedriver + 4691870
6   chromedriver                        0x000000010068e961 chromedriver + 4712801
7   chromedriver                        0x00000001006952ff chromedriver + 4739839
8   chromedriver                        0x000000010068f85a chromedriver + 4716634
9   chromedriver                        0x0000000100661fce chromedriver + 4530126
10  chromedriver                        0x00000001006af5c8 chromedriver + 4847048
11  chromedriver                        0x00000001006af747 chromedriver + 4847431
12  chromedriver                        0x00000001006c487f chromedriver + 4933759
13  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
14  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:47:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:47:11 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:47:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 661,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 85349,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 107.38725,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 47, 11, 104091),
 'httpcompression/response_bytes': 269041,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 133685248,
 'memusage/startup': 129077248,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSessionIdException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 45, 23, 716841)}
2023-03-28 13:47:11 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:47:12 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:47:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 121, in get_company_name_MDN
    self.wait.until(EC.visibility_of_element_located((By.ID, 'pt2:t2:0:c3')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x00000001036dc428 chromedriver + 4899880
1   chromedriver                        0x0000000103659a23 chromedriver + 4364835
2   chromedriver                        0x00000001032a3bf6 chromedriver + 474102
3   chromedriver                        0x00000001032e74f0 chromedriver + 750832
4   chromedriver                        0x00000001032e7751 chromedriver + 751441
5   chromedriver                        0x000000010332b834 chromedriver + 1030196
6   chromedriver                        0x000000010330d58d chromedriver + 906637
7   chromedriver                        0x0000000103328b5b chromedriver + 1018715
8   chromedriver                        0x000000010330d333 chromedriver + 906035
9   chromedriver                        0x00000001032d755f chromedriver + 685407
10  chromedriver                        0x00000001032d8a7e chromedriver + 690814
11  chromedriver                        0x00000001036a979e chromedriver + 4691870
12  chromedriver                        0x00000001036ae961 chromedriver + 4712801
13  chromedriver                        0x00000001036b52ff chromedriver + 4739839
14  chromedriver                        0x00000001036af85a chromedriver + 4716634
15  chromedriver                        0x0000000103681fce chromedriver + 4530126
16  chromedriver                        0x00000001036cf5c8 chromedriver + 4847048
17  chromedriver                        0x00000001036cf747 chromedriver + 4847431
18  chromedriver                        0x00000001036e487f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 78, in get_company_finance_data
    company_name, company_MDN = self.get_company_name_MDN()
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 123, in get_company_name_MDN
    if self.driver.find_element(By.ID, 'pt2:t2::emptyTxt'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="pt2:t2::emptyTxt"]"}
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x00000001036dc428 chromedriver + 4899880
1   chromedriver                        0x0000000103659a23 chromedriver + 4364835
2   chromedriver                        0x00000001032a3bf6 chromedriver + 474102
3   chromedriver                        0x00000001032e74f0 chromedriver + 750832
4   chromedriver                        0x00000001032e7751 chromedriver + 751441
5   chromedriver                        0x000000010332b834 chromedriver + 1030196
6   chromedriver                        0x000000010330d58d chromedriver + 906637
7   chromedriver                        0x0000000103328b5b chromedriver + 1018715
8   chromedriver                        0x000000010330d333 chromedriver + 906035
9   chromedriver                        0x00000001032d755f chromedriver + 685407
10  chromedriver                        0x00000001032d8a7e chromedriver + 690814
11  chromedriver                        0x00000001036a979e chromedriver + 4691870
12  chromedriver                        0x00000001036ae961 chromedriver + 4712801
13  chromedriver                        0x00000001036b52ff chromedriver + 4739839
14  chromedriver                        0x00000001036af85a chromedriver + 4716634
15  chromedriver                        0x0000000103681fce chromedriver + 4530126
16  chromedriver                        0x00000001036cf5c8 chromedriver + 4847048
17  chromedriver                        0x00000001036cf747 chromedriver + 4847431
18  chromedriver                        0x00000001036e487f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 91, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
0   chromedriver                        0x00000001036dc428 chromedriver + 4899880
1   chromedriver                        0x0000000103659a23 chromedriver + 4364835
2   chromedriver                        0x00000001032a3a71 chromedriver + 473713
3   chromedriver                        0x00000001032d7105 chromedriver + 684293
4   chromedriver                        0x00000001032d8a7e chromedriver + 690814
5   chromedriver                        0x00000001036a979e chromedriver + 4691870
6   chromedriver                        0x00000001036ae961 chromedriver + 4712801
7   chromedriver                        0x00000001036b52ff chromedriver + 4739839
8   chromedriver                        0x00000001036af85a chromedriver + 4716634
9   chromedriver                        0x0000000103681fce chromedriver + 4530126
10  chromedriver                        0x00000001036cf5c8 chromedriver + 4847048
11  chromedriver                        0x00000001036cf747 chromedriver + 4847431
12  chromedriver                        0x00000001036e487f chromedriver + 4933759
13  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
14  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:47:12 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:47:12 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:47:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 646,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 85348,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 108.352082,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 47, 12, 289569),
 'httpcompression/response_bytes': 269051,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 134524928,
 'memusage/startup': 129908736,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSessionIdException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 45, 23, 937487)}
2023-03-28 13:47:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:47:15 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:47:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 70, in get_company_finance_data
    self.wait_until_done()
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 103, in wait_until_done
    self.wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 69, in _predicate
    return driver.find_element(*locator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000100530428 chromedriver + 4899880
1   chromedriver                        0x00000001004ada23 chromedriver + 4364835
2   chromedriver                        0x00000001000f7bf6 chromedriver + 474102
3   chromedriver                        0x00000001000cdcdc chromedriver + 302300
4   chromedriver                        0x000000010016685f chromedriver + 927839
5   chromedriver                        0x000000010017c226 chromedriver + 1016358
6   chromedriver                        0x0000000100161333 chromedriver + 906035
7   chromedriver                        0x000000010012b55f chromedriver + 685407
8   chromedriver                        0x000000010012ca7e chromedriver + 690814
9   chromedriver                        0x00000001004fd79e chromedriver + 4691870
10  chromedriver                        0x0000000100502961 chromedriver + 4712801
11  chromedriver                        0x00000001005092ff chromedriver + 4739839
12  chromedriver                        0x000000010050385a chromedriver + 4716634
13  chromedriver                        0x00000001004d5fce chromedriver + 4530126
14  chromedriver                        0x00000001005235c8 chromedriver + 4847048
15  chromedriver                        0x0000000100523747 chromedriver + 4847431
16  chromedriver                        0x000000010053887f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 90, in get_company_finance_data
    self.driver.close()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 551, in close
    self.execute(Command.CLOSE)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000100530428 chromedriver + 4899880
1   chromedriver                        0x00000001004ada23 chromedriver + 4364835
2   chromedriver                        0x00000001000f7bf6 chromedriver + 474102
3   chromedriver                        0x00000001000cdcdc chromedriver + 302300
4   chromedriver                        0x000000010016685f chromedriver + 927839
5   chromedriver                        0x000000010016d5ac chromedriver + 955820
6   chromedriver                        0x0000000100161528 chromedriver + 906536
7   chromedriver                        0x000000010012b55f chromedriver + 685407
8   chromedriver                        0x000000010012ca7e chromedriver + 690814
9   chromedriver                        0x00000001004fd79e chromedriver + 4691870
10  chromedriver                        0x0000000100502961 chromedriver + 4712801
11  chromedriver                        0x00000001005092ff chromedriver + 4739839
12  chromedriver                        0x000000010050385a chromedriver + 4716634
13  chromedriver                        0x00000001004d5fce chromedriver + 4530126
14  chromedriver                        0x00000001005235c8 chromedriver + 4847048
15  chromedriver                        0x0000000100523747 chromedriver + 4847431
16  chromedriver                        0x000000010053887f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:47:15 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:47:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 659,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 48764,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 111.766973,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 47, 15, 705753),
 'httpcompression/response_bytes': 148606,
 'httpcompression/response_count': 2,
 'item_scraped_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 141357056,
 'memusage/startup': 129409024,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 45, 23, 938780)}
2023-03-28 13:47:15 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:50:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:50:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:50:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 13:50:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:50:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:50:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 13:50:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:50:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:50:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 13:50:25 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:50:25 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:50:25 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 13:50:25 [scrapy.extensions.telnet] INFO: Telnet Password: 770e8d8cec6ca73d
2023-03-28 13:50:25 [scrapy.extensions.telnet] INFO: Telnet Password: 51d0c9475bf20bf5
2023-03-28 13:50:25 [scrapy.extensions.telnet] INFO: Telnet Password: 0b25f351a4b52451
2023-03-28 13:50:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:50:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:50:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 13:50:25 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:50:25 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:50:25 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 13:50:25 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:50:25 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:50:25 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 13:50:25 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:50:25 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:50:25 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 13:50:29 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:50:29 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:50:29 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:50:29 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 13:50:29 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:50:29 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:50:29 [scrapy.core.engine] INFO: Spider opened
2023-03-28 13:50:30 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:50:31 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:50:31 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 13:50:36 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:50:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:50:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-28 13:50:36 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:50:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:50:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-28 13:50:36 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 13:50:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 13:50:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-28 13:51:06 [root] INFO: First page:1264, Max page:2526
2023-03-28 13:51:12 [root] INFO: First page:1, Max page:1263
2023-03-28 13:51:15 [root] INFO: First page:2527, Max page:3791
2023-03-28 13:51:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 13:52:00 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 13:52:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 2 items (at 2 items/min)
2023-03-28 13:52:39 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2023-03-28 13:52:42 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2023-03-28 13:53:00 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 3 items (at 2 items/min)
2023-03-28 13:53:38 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 2 items/min)
2023-03-28 13:53:46 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:53:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 87, in get_company_finance_data
    self.get_report_data(year=year, quarter=quarter, report_name = report_name, company_name=company_name, company_MDN=company_MDN)
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 158, in get_report_data
    table_link.click()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py", line 93, in click
    self._execute(Command.CLICK_ELEMENT)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py", line 403, in _execute
    return self._parent.execute(command, params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.ElementClickInterceptedException: Message: element click intercepted: Element <a role="tab" aria-controls="pt2:tab2::body" id="pt2:tab2::disAcr" class="x13v" data-afr-fcs="true" tabindex="-1" href="#" onclick="return false;">...</a> is not clickable at point (91, 567). Other element would receive the click: <td nowrap="" role="gridcell" id="pt2:tt1:8:c1" class="xib rowMark" _afrndcol="1">...</td>
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010d346428 chromedriver + 4899880
1   chromedriver                        0x000000010d2c3a23 chromedriver + 4364835
2   chromedriver                        0x000000010cf0dbf6 chromedriver + 474102
3   chromedriver                        0x000000010cf5ad81 chromedriver + 789889
4   chromedriver                        0x000000010cf57dde chromedriver + 777694
5   chromedriver                        0x000000010cf54a54 chromedriver + 764500
6   chromedriver                        0x000000010cf5343f chromedriver + 758847
7   chromedriver                        0x000000010cf43cf3 chromedriver + 695539
8   chromedriver                        0x000000010cf77562 chromedriver + 906594
9   chromedriver                        0x000000010cf43361 chromedriver + 693089
10  chromedriver                        0x000000010cf7771e chromedriver + 907038
11  chromedriver                        0x000000010cf92b5b chromedriver + 1018715
12  chromedriver                        0x000000010cf77333 chromedriver + 906035
13  chromedriver                        0x000000010cf4155f chromedriver + 685407
14  chromedriver                        0x000000010cf42a7e chromedriver + 690814
15  chromedriver                        0x000000010d31379e chromedriver + 4691870
16  chromedriver                        0x000000010d318961 chromedriver + 4712801
17  chromedriver                        0x000000010d31f2ff chromedriver + 4739839
18  chromedriver                        0x000000010d31985a chromedriver + 4716634
19  chromedriver                        0x000000010d2ebfce chromedriver + 4530126
20  chromedriver                        0x000000010d3395c8 chromedriver + 4847048
21  chromedriver                        0x000000010d339747 chromedriver + 4847431
22  chromedriver                        0x000000010d34e87f chromedriver + 4933759
23  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
24  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 94, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
0   chromedriver                        0x000000010d346428 chromedriver + 4899880
1   chromedriver                        0x000000010d2c3a23 chromedriver + 4364835
2   chromedriver                        0x000000010cf0da71 chromedriver + 473713
3   chromedriver                        0x000000010cf41105 chromedriver + 684293
4   chromedriver                        0x000000010cf42a7e chromedriver + 690814
5   chromedriver                        0x000000010d31379e chromedriver + 4691870
6   chromedriver                        0x000000010d318961 chromedriver + 4712801
7   chromedriver                        0x000000010d31f2ff chromedriver + 4739839
8   chromedriver                        0x000000010d31985a chromedriver + 4716634
9   chromedriver                        0x000000010d2ebfce chromedriver + 4530126
10  chromedriver                        0x000000010d3395c8 chromedriver + 4847048
11  chromedriver                        0x000000010d339747 chromedriver + 4847431
12  chromedriver                        0x000000010d34e87f chromedriver + 4933759
13  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
14  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:53:46 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2023-03-28 13:53:46 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:53:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 512,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5677,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 189.859216,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 53, 46, 358837),
 'httpcompression/response_bytes': 11703,
 'httpcompression/response_count': 2,
 'item_scraped_count': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 19,
 'log_count/WARNING': 3,
 'memusage/max': 142475264,
 'memusage/startup': 129376256,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSessionIdException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 50, 36, 499621)}
2023-03-28 13:53:46 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:54:04 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 2 items/min)
2023-03-28 13:55:03 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 8 items (at 3 items/min)
2023-03-28 13:55:57 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 6 items (at 1 items/min)
2023-03-28 13:55:58 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 10 items (at 2 items/min)
2023-03-28 13:56:27 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:56:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 87, in get_company_finance_data
    self.get_report_data(year=year, quarter=quarter, report_name = report_name, company_name=company_name, company_MDN=company_MDN)
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 158, in get_report_data
    table_link.click()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py", line 93, in click
    self._execute(Command.CLICK_ELEMENT)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py", line 403, in _execute
    return self._parent.execute(command, params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.ElementClickInterceptedException: Message: element click intercepted: Element <a role="tab" aria-controls="pt2:tab2::body" id="pt2:tab2::disAcr" class="x13v" data-afr-fcs="true" tabindex="-1" href="#" onclick="return false;">...</a> is not clickable at point (91, 565). Other element would receive the click: <td nowrap="" role="gridcell" id="pt2:tt1:9:c1" class="xia rowMark" _afrndcol="1">...</td>
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000101452428 chromedriver + 4899880
1   chromedriver                        0x00000001013cfa23 chromedriver + 4364835
2   chromedriver                        0x0000000101019bf6 chromedriver + 474102
3   chromedriver                        0x0000000101066d81 chromedriver + 789889
4   chromedriver                        0x0000000101063dde chromedriver + 777694
5   chromedriver                        0x0000000101060a54 chromedriver + 764500
6   chromedriver                        0x000000010105f43f chromedriver + 758847
7   chromedriver                        0x000000010104fcf3 chromedriver + 695539
8   chromedriver                        0x0000000101083562 chromedriver + 906594
9   chromedriver                        0x000000010104f361 chromedriver + 693089
10  chromedriver                        0x000000010108371e chromedriver + 907038
11  chromedriver                        0x000000010109eb5b chromedriver + 1018715
12  chromedriver                        0x0000000101083333 chromedriver + 906035
13  chromedriver                        0x000000010104d55f chromedriver + 685407
14  chromedriver                        0x000000010104ea7e chromedriver + 690814
15  chromedriver                        0x000000010141f79e chromedriver + 4691870
16  chromedriver                        0x0000000101424961 chromedriver + 4712801
17  chromedriver                        0x000000010142b2ff chromedriver + 4739839
18  chromedriver                        0x000000010142585a chromedriver + 4716634
19  chromedriver                        0x00000001013f7fce chromedriver + 4530126
20  chromedriver                        0x00000001014455c8 chromedriver + 4847048
21  chromedriver                        0x0000000101445747 chromedriver + 4847431
22  chromedriver                        0x000000010145a87f chromedriver + 4933759
23  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
24  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 94, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
0   chromedriver                        0x0000000101452428 chromedriver + 4899880
1   chromedriver                        0x00000001013cfa23 chromedriver + 4364835
2   chromedriver                        0x0000000101019a71 chromedriver + 473713
3   chromedriver                        0x000000010104d105 chromedriver + 684293
4   chromedriver                        0x000000010104ea7e chromedriver + 690814
5   chromedriver                        0x000000010141f79e chromedriver + 4691870
6   chromedriver                        0x0000000101424961 chromedriver + 4712801
7   chromedriver                        0x000000010142b2ff chromedriver + 4739839
8   chromedriver                        0x000000010142585a chromedriver + 4716634
9   chromedriver                        0x00000001013f7fce chromedriver + 4530126
10  chromedriver                        0x00000001014455c8 chromedriver + 4847048
11  chromedriver                        0x0000000101445747 chromedriver + 4847431
12  chromedriver                        0x000000010145a87f chromedriver + 4933759
13  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
14  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:56:28 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:56:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 499,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5671,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 351.564278,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 56, 28, 67210),
 'httpcompression/response_bytes': 11703,
 'httpcompression/response_count': 2,
 'item_scraped_count': 10,
 'log_count/ERROR': 1,
 'log_count/INFO': 21,
 'log_count/WARNING': 3,
 'memusage/max': 144719872,
 'memusage/startup': 128917504,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSessionIdException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 50, 36, 502932)}
2023-03-28 13:56:28 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 13:56:51 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 8 items (at 2 items/min)
2023-03-28 13:57:48 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 10 items (at 2 items/min)
2023-03-28 13:58:40 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 12 items (at 2 items/min)
2023-03-28 13:59:44 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 13:59:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 71, in get_company_finance_data
    self.wait_until_done()
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 106, in wait_until_done
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 69, in _predicate
    return driver.find_element(*locator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010ee4f428 chromedriver + 4899880
1   chromedriver                        0x000000010edcca23 chromedriver + 4364835
2   chromedriver                        0x000000010ea16bf6 chromedriver + 474102
3   chromedriver                        0x000000010e9eccdc chromedriver + 302300
4   chromedriver                        0x000000010ea8585f chromedriver + 927839
5   chromedriver                        0x000000010ea9b226 chromedriver + 1016358
6   chromedriver                        0x000000010ea80333 chromedriver + 906035
7   chromedriver                        0x000000010ea4a55f chromedriver + 685407
8   chromedriver                        0x000000010ea4ba7e chromedriver + 690814
9   chromedriver                        0x000000010ee1c79e chromedriver + 4691870
10  chromedriver                        0x000000010ee21961 chromedriver + 4712801
11  chromedriver                        0x000000010ee282ff chromedriver + 4739839
12  chromedriver                        0x000000010ee2285a chromedriver + 4716634
13  chromedriver                        0x000000010edf4fce chromedriver + 4530126
14  chromedriver                        0x000000010ee425c8 chromedriver + 4847048
15  chromedriver                        0x000000010ee42747 chromedriver + 4847431
16  chromedriver                        0x000000010ee5787f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
    ^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 551, in close
    self.execute(Command.CLOSE)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010ee4f428 chromedriver + 4899880
1   chromedriver                        0x000000010edcca23 chromedriver + 4364835
2   chromedriver                        0x000000010ea16bf6 chromedriver + 474102
3   chromedriver                        0x000000010e9eccdc chromedriver + 302300
4   chromedriver                        0x000000010ea8585f chromedriver + 927839
5   chromedriver                        0x000000010ea8c5ac chromedriver + 955820
6   chromedriver                        0x000000010ea80528 chromedriver + 906536
7   chromedriver                        0x000000010ea4a55f chromedriver + 685407
8   chromedriver                        0x000000010ea4ba7e chromedriver + 690814
9   chromedriver                        0x000000010ee1c79e chromedriver + 4691870
10  chromedriver                        0x000000010ee21961 chromedriver + 4712801
11  chromedriver                        0x000000010ee282ff chromedriver + 4739839
12  chromedriver                        0x000000010ee2285a chromedriver + 4716634
13  chromedriver                        0x000000010edf4fce chromedriver + 4530126
14  chromedriver                        0x000000010ee425c8 chromedriver + 4847048
15  chromedriver                        0x000000010ee42747 chromedriver + 4847431
16  chromedriver                        0x000000010ee5787f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 13:59:44 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 14 items (at 2 items/min)
2023-03-28 13:59:44 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 13:59:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 607,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 85883,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 548.426507,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 5, 59, 44, 675653),
 'httpcompression/response_bytes': 269030,
 'httpcompression/response_count': 2,
 'item_scraped_count': 14,
 'log_count/ERROR': 1,
 'log_count/INFO': 24,
 'log_count/WARNING': 3,
 'memusage/max': 146620416,
 'memusage/startup': 129249280,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 5, 50, 36, 249146)}
2023-03-28 13:59:44 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:00:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:00:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:00:25 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:00:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:00:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:00:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:00:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:00:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:00:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:00:25 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:00:25 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:00:25 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:00:25 [scrapy.extensions.telnet] INFO: Telnet Password: c89a65e568860e5e
2023-03-28 14:00:25 [scrapy.extensions.telnet] INFO: Telnet Password: eb14d8533d6d3b09
2023-03-28 14:00:25 [scrapy.extensions.telnet] INFO: Telnet Password: f1265395c9d3f8c6
2023-03-28 14:00:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:00:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:00:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:00:25 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:00:25 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:00:25 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:00:25 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:00:25 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:00:25 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:00:26 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:00:26 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:00:26 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:00:29 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:00:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:00:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:00:29 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:00:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:00:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:00:29 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:00:29 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:00:29 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:00:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:00:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:00:30 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:00:30 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:00:30 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:00:30 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:00:31 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:00:31 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:00:31 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:00:36 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:00:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:00:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-28 14:00:36 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:00:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:00:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-28 14:00:36 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:00:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:00:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-28 14:01:14 [root] INFO: First page:1264, Max page:2526
2023-03-28 14:01:22 [root] INFO: First page:1, Max page:1263
2023-03-28 14:01:27 [root] INFO: First page:2527, Max page:3791
2023-03-28 14:01:54 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 14:02:00 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 14:02:46 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2023-03-28 14:02:51 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:03:30 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:03:46 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 14:03:58 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 2 items/min)
2023-03-28 14:04:03 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2023-03-28 14:04:44 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 3 items (at 2 items/min)
2023-03-28 14:05:08 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 2 items/min)
2023-03-28 14:05:37 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 1 items/min)
2023-03-28 14:05:48 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 2 items/min)
2023-03-28 14:06:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 6 items (at 2 items/min)
2023-03-28 14:06:39 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 7 items (at 2 items/min)
2023-03-28 14:06:49 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 7 items (at 2 items/min)
2023-03-28 14:06:53 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 7 items (at 1 items/min)
2023-03-28 14:07:46 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 9 items (at 2 items/min)
2023-03-28 14:07:54 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 9 items (at 2 items/min)
2023-03-28 14:08:50 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 11 items (at 2 items/min)
2023-03-28 14:08:51 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 11 items (at 2 items/min)
2023-03-28 14:08:58 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:09:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 9 items (at 2 items/min)
2023-03-28 14:09:50 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 13 items (at 2 items/min)
2023-03-28 14:09:56 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 13 items (at 2 items/min)
2023-03-28 14:10:03 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 10 items (at 1 items/min)
2023-03-28 14:10:48 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 15 items (at 2 items/min)
2023-03-28 14:11:00 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 15 items (at 2 items/min)
2023-03-28 14:11:02 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 12 items (at 2 items/min)
2023-03-28 14:11:41 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 13 items (at 1 items/min)
2023-03-28 14:11:50 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 16 items (at 1 items/min)
2023-03-28 14:11:51 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 17 items (at 2 items/min)
2023-03-28 14:12:40 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 15 items (at 2 items/min)
2023-03-28 14:12:55 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 19 items (at 2 items/min)
2023-03-28 14:13:08 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 18 items (at 2 items/min)
2023-03-28 14:13:42 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:13:54 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 17 items (at 2 items/min)
2023-03-28 14:13:54 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 21 items (at 2 items/min)
2023-03-28 14:14:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 19 items (at 1 items/min)
2023-03-28 14:15:04 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 23 items (at 2 items/min)
2023-03-28 14:15:08 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 20 items (at 1 items/min)
2023-03-28 14:15:09 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 19 items (at 2 items/min)
2023-03-28 14:15:29 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:15:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 74, in get_company_finance_data
    self.wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'xgl')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 139, in _predicate
    return _element_if_visible(driver.find_element(*locator))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000104144428 chromedriver + 4899880
1   chromedriver                        0x00000001040c1a23 chromedriver + 4364835
2   chromedriver                        0x0000000103d0bbf6 chromedriver + 474102
3   chromedriver                        0x0000000103ce1cdc chromedriver + 302300
4   chromedriver                        0x0000000103d7a85f chromedriver + 927839
5   chromedriver                        0x0000000103d90226 chromedriver + 1016358
6   chromedriver                        0x0000000103d75333 chromedriver + 906035
7   chromedriver                        0x0000000103d3f55f chromedriver + 685407
8   chromedriver                        0x0000000103d40a7e chromedriver + 690814
9   chromedriver                        0x000000010411179e chromedriver + 4691870
10  chromedriver                        0x0000000104116961 chromedriver + 4712801
11  chromedriver                        0x000000010411d2ff chromedriver + 4739839
12  chromedriver                        0x000000010411785a chromedriver + 4716634
13  chromedriver                        0x00000001040e9fce chromedriver + 4530126
14  chromedriver                        0x00000001041375c8 chromedriver + 4847048
15  chromedriver                        0x0000000104137747 chromedriver + 4847431
16  chromedriver                        0x000000010414c87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000104144428 chromedriver + 4899880
1   chromedriver                        0x00000001040c1a23 chromedriver + 4364835
2   chromedriver                        0x0000000103d0bbf6 chromedriver + 474102
3   chromedriver                        0x0000000103ce1cdc chromedriver + 302300
4   chromedriver                        0x0000000103d7a85f chromedriver + 927839
5   chromedriver                        0x0000000103d90226 chromedriver + 1016358
6   chromedriver                        0x0000000103d75333 chromedriver + 906035
7   chromedriver                        0x0000000103d3f55f chromedriver + 685407
8   chromedriver                        0x0000000103d40a7e chromedriver + 690814
9   chromedriver                        0x000000010411179e chromedriver + 4691870
10  chromedriver                        0x0000000104116961 chromedriver + 4712801
11  chromedriver                        0x000000010411d2ff chromedriver + 4739839
12  chromedriver                        0x000000010411785a chromedriver + 4716634
13  chromedriver                        0x00000001040e9fce chromedriver + 4530126
14  chromedriver                        0x00000001041375c8 chromedriver + 4847048
15  chromedriver                        0x0000000104137747 chromedriver + 4847431
16  chromedriver                        0x000000010414c87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:15:29 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:15:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 578,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 8255,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 892.674817,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 15, 29, 240289),
 'httpcompression/response_bytes': 7240,
 'httpcompression/response_count': 1,
 'item_scraped_count': 23,
 'log_count/ERROR': 1,
 'log_count/INFO': 29,
 'log_count/WARNING': 3,
 'memusage/max': 145317888,
 'memusage/startup': 129798144,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 0, 36, 565472)}
2023-03-28 14:15:29 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:15:32 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:15:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 74, in get_company_finance_data
    self.wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'xgl')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 139, in _predicate
    return _element_if_visible(driver.find_element(*locator))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010ac08428 chromedriver + 4899880
1   chromedriver                        0x000000010ab85a23 chromedriver + 4364835
2   chromedriver                        0x000000010a7cfbf6 chromedriver + 474102
3   chromedriver                        0x000000010a7a5cdc chromedriver + 302300
4   chromedriver                        0x000000010a83e85f chromedriver + 927839
5   chromedriver                        0x000000010a854226 chromedriver + 1016358
6   chromedriver                        0x000000010a839333 chromedriver + 906035
7   chromedriver                        0x000000010a80355f chromedriver + 685407
8   chromedriver                        0x000000010a804a7e chromedriver + 690814
9   chromedriver                        0x000000010abd579e chromedriver + 4691870
10  chromedriver                        0x000000010abda961 chromedriver + 4712801
11  chromedriver                        0x000000010abe12ff chromedriver + 4739839
12  chromedriver                        0x000000010abdb85a chromedriver + 4716634
13  chromedriver                        0x000000010abadfce chromedriver + 4530126
14  chromedriver                        0x000000010abfb5c8 chromedriver + 4847048
15  chromedriver                        0x000000010abfb747 chromedriver + 4847431
16  chromedriver                        0x000000010ac1087f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010ac08428 chromedriver + 4899880
1   chromedriver                        0x000000010ab85a23 chromedriver + 4364835
2   chromedriver                        0x000000010a7cfbf6 chromedriver + 474102
3   chromedriver                        0x000000010a7a5cdc chromedriver + 302300
4   chromedriver                        0x000000010a83e85f chromedriver + 927839
5   chromedriver                        0x000000010a854226 chromedriver + 1016358
6   chromedriver                        0x000000010a839333 chromedriver + 906035
7   chromedriver                        0x000000010a80355f chromedriver + 685407
8   chromedriver                        0x000000010a804a7e chromedriver + 690814
9   chromedriver                        0x000000010abd579e chromedriver + 4691870
10  chromedriver                        0x000000010abda961 chromedriver + 4712801
11  chromedriver                        0x000000010abe12ff chromedriver + 4739839
12  chromedriver                        0x000000010abdb85a chromedriver + 4716634
13  chromedriver                        0x000000010abadfce chromedriver + 4530126
14  chromedriver                        0x000000010abfb5c8 chromedriver + 4847048
15  chromedriver                        0x000000010abfb747 chromedriver + 4847431
16  chromedriver                        0x000000010ac1087f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:15:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:15:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 502,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5675,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 895.78202,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 15, 32, 645825),
 'httpcompression/response_bytes': 11703,
 'httpcompression/response_count': 2,
 'item_scraped_count': 19,
 'log_count/ERROR': 1,
 'log_count/INFO': 27,
 'log_count/WARNING': 5,
 'memusage/max': 142819328,
 'memusage/startup': 129073152,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 0, 36, 863805)}
2023-03-28 14:15:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:15:32 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:15:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 74, in get_company_finance_data
    self.wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'xgl')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 139, in _predicate
    return _element_if_visible(driver.find_element(*locator))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000107ab3428 chromedriver + 4899880
1   chromedriver                        0x0000000107a30a23 chromedriver + 4364835
2   chromedriver                        0x000000010767abf6 chromedriver + 474102
3   chromedriver                        0x0000000107650cdc chromedriver + 302300
4   chromedriver                        0x00000001076e985f chromedriver + 927839
5   chromedriver                        0x00000001076ff226 chromedriver + 1016358
6   chromedriver                        0x00000001076e4333 chromedriver + 906035
7   chromedriver                        0x00000001076ae55f chromedriver + 685407
8   chromedriver                        0x00000001076afa7e chromedriver + 690814
9   chromedriver                        0x0000000107a8079e chromedriver + 4691870
10  chromedriver                        0x0000000107a85961 chromedriver + 4712801
11  chromedriver                        0x0000000107a8c2ff chromedriver + 4739839
12  chromedriver                        0x0000000107a8685a chromedriver + 4716634
13  chromedriver                        0x0000000107a58fce chromedriver + 4530126
14  chromedriver                        0x0000000107aa65c8 chromedriver + 4847048
15  chromedriver                        0x0000000107aa6747 chromedriver + 4847431
16  chromedriver                        0x0000000107abb87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000107ab3428 chromedriver + 4899880
1   chromedriver                        0x0000000107a30a23 chromedriver + 4364835
2   chromedriver                        0x000000010767abf6 chromedriver + 474102
3   chromedriver                        0x0000000107650cdc chromedriver + 302300
4   chromedriver                        0x00000001076e985f chromedriver + 927839
5   chromedriver                        0x00000001076ff226 chromedriver + 1016358
6   chromedriver                        0x00000001076e4333 chromedriver + 906035
7   chromedriver                        0x00000001076ae55f chromedriver + 685407
8   chromedriver                        0x00000001076afa7e chromedriver + 690814
9   chromedriver                        0x0000000107a8079e chromedriver + 4691870
10  chromedriver                        0x0000000107a85961 chromedriver + 4712801
11  chromedriver                        0x0000000107a8c2ff chromedriver + 4739839
12  chromedriver                        0x0000000107a8685a chromedriver + 4716634
13  chromedriver                        0x0000000107a58fce chromedriver + 4530126
14  chromedriver                        0x0000000107aa65c8 chromedriver + 4847048
15  chromedriver                        0x0000000107aa6747 chromedriver + 4847431
16  chromedriver                        0x0000000107abb87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:15:32 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:15:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 632,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5711,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 896.14113,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 15, 32, 965038),
 'httpcompression/response_bytes': 11770,
 'httpcompression/response_count': 2,
 'item_scraped_count': 20,
 'log_count/ERROR': 1,
 'log_count/INFO': 29,
 'log_count/WARNING': 5,
 'memusage/max': 144920576,
 'memusage/startup': 128729088,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 0, 36, 823908)}
2023-03-28 14:15:32 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:17:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:17:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:17:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:17:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:17:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:17:39 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:17:39 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:17:39 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:17:39 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:17:39 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:17:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:17:39 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:17:39 [scrapy.extensions.telnet] INFO: Telnet Password: 489b805c4c8a4e87
2023-03-28 14:17:39 [scrapy.extensions.telnet] INFO: Telnet Password: 6d30739f16fdf1ec
2023-03-28 14:17:39 [scrapy.extensions.telnet] INFO: Telnet Password: ecc3262dcde21645
2023-03-28 14:17:39 [scrapy.extensions.telnet] INFO: Telnet Password: 5e0b0bdb5853a542
2023-03-28 14:17:39 [scrapy.extensions.telnet] INFO: Telnet Password: a14c7d1f124f23b9
2023-03-28 14:17:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:17:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:17:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:17:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:17:39 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:17:39 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:17:39 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:17:39 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:17:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:17:39 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:17:40 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:17:40 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:17:40 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:17:40 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:17:40 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:17:40 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:17:40 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:17:40 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:17:40 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:17:40 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:17:44 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:17:44 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:17:44 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:17:44 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:17:44 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:17:44 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:17:45 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:17:45 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:17:45 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:17:45 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:17:45 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:17:45 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:17:45 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:17:46 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:17:46 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:17:46 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:17:46 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:17:46 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:17:51 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:17:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:17:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-28 14:17:51 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:17:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:17:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-28 14:17:52 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:17:52 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:17:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:17:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:17:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2023-03-28 14:17:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-28 14:17:52 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:17:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:17:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6027
2023-03-28 14:17:53 [root] INFO: First page:1517, Max page:2274
2023-03-28 14:18:50 [root] INFO: First page:3033, Max page:3791
2023-03-28 14:18:51 [root] INFO: First page:1, Max page:758
2023-03-28 14:18:53 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:19:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 14:19:30 [root] INFO: First page:759, Max page:1516
2023-03-28 14:19:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:19:44 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 14:19:55 [root] WARNING: Error occurred: Search Wrong Retry Times#2
2023-03-28 14:20:04 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 14:20:08 [root] INFO: First page:2275, Max page:3032
2023-03-28 14:20:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:20:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2023-03-28 14:20:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2023-03-28 14:20:57 [root] WARNING: Error occurred: Search Wrong Retry Times#3
2023-03-28 14:20:58 [root] ERROR: Error occurred: Retry Enough Times
2023-03-28 14:20:58 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:20:58 [twisted] CRITICAL: Unhandled error in Deferred:
2023-03-28 14:20:58 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/task.py", line 526, in _oneWorkUnit
    result = next(self._iterator)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 94, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 99, in get_company_finance_data
    sys.exit()
SystemExit
2023-03-28 14:20:58 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:20:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 632,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 48154,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 186.639146,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 20, 58, 640667),
 'httpcompression/response_bytes': 146908,
 'httpcompression/response_count': 2,
 'log_count/CRITICAL': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 5,
 'memusage/max': 133627904,
 'memusage/startup': 129744896,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 17, 52, 1521)}
2023-03-28 14:20:58 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:20:59 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2023-03-28 14:21:07 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 2 items (at 2 items/min)
2023-03-28 14:21:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2023-03-28 14:22:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 1 items/min)
2023-03-28 14:22:16 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 2 items/min)
2023-03-28 14:22:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 2 items/min)
2023-03-28 14:22:22 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:22:52 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 3 items (at 2 items/min)
2023-03-28 14:22:54 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 6 items (at 1 items/min)
2023-03-28 14:23:05 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 1 items/min)
2023-03-28 14:23:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 6 items (at 2 items/min)
2023-03-28 14:24:01 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 2 items/min)
2023-03-28 14:24:07 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 7 items (at 1 items/min)
2023-03-28 14:24:13 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 8 items (at 2 items/min)
2023-03-28 14:24:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 7 items (at 2 items/min)
2023-03-28 14:24:50 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:24:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 71, in get_company_finance_data
    self.wait_until_done()
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 105, in wait_until_done
    self.wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 69, in _predicate
    return driver.find_element(*locator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000107bff428 chromedriver + 4899880
1   chromedriver                        0x0000000107b7ca23 chromedriver + 4364835
2   chromedriver                        0x00000001077c6bf6 chromedriver + 474102
3   chromedriver                        0x000000010779ccdc chromedriver + 302300
4   chromedriver                        0x000000010783585f chromedriver + 927839
5   chromedriver                        0x000000010784b226 chromedriver + 1016358
6   chromedriver                        0x0000000107830333 chromedriver + 906035
7   chromedriver                        0x00000001077fa55f chromedriver + 685407
8   chromedriver                        0x00000001077fba7e chromedriver + 690814
9   chromedriver                        0x0000000107bcc79e chromedriver + 4691870
10  chromedriver                        0x0000000107bd1961 chromedriver + 4712801
11  chromedriver                        0x0000000107bd82ff chromedriver + 4739839
12  chromedriver                        0x0000000107bd285a chromedriver + 4716634
13  chromedriver                        0x0000000107ba4fce chromedriver + 4530126
14  chromedriver                        0x0000000107bf25c8 chromedriver + 4847048
15  chromedriver                        0x0000000107bf2747 chromedriver + 4847431
16  chromedriver                        0x0000000107c0787f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000107bff428 chromedriver + 4899880
1   chromedriver                        0x0000000107b7ca23 chromedriver + 4364835
2   chromedriver                        0x00000001077c6bf6 chromedriver + 474102
3   chromedriver                        0x000000010779ccdc chromedriver + 302300
4   chromedriver                        0x000000010783585f chromedriver + 927839
5   chromedriver                        0x000000010784b226 chromedriver + 1016358
6   chromedriver                        0x0000000107830333 chromedriver + 906035
7   chromedriver                        0x00000001077fa55f chromedriver + 685407
8   chromedriver                        0x00000001077fba7e chromedriver + 690814
9   chromedriver                        0x0000000107bcc79e chromedriver + 4691870
10  chromedriver                        0x0000000107bd1961 chromedriver + 4712801
11  chromedriver                        0x0000000107bd82ff chromedriver + 4739839
12  chromedriver                        0x0000000107bd285a chromedriver + 4716634
13  chromedriver                        0x0000000107ba4fce chromedriver + 4530126
14  chromedriver                        0x0000000107bf25c8 chromedriver + 4847048
15  chromedriver                        0x0000000107bf2747 chromedriver + 4847431
16  chromedriver                        0x0000000107c0787f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:24:50 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:24:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 620,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 80963,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 419.226334,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 24, 50, 903909),
 'httpcompression/response_bytes': 259362,
 'httpcompression/response_count': 2,
 'item_scraped_count': 8,
 'log_count/ERROR': 1,
 'log_count/INFO': 21,
 'log_count/WARNING': 3,
 'memusage/max': 128745472,
 'memusage/startup': 128745472,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 17, 51, 677575)}
2023-03-28 14:24:50 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:24:51 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:24:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 123, in get_company_name_MDN
    self.wait.until(EC.visibility_of_element_located((By.ID, 'pt2:t2:0:c3')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 139, in _predicate
    return _element_if_visible(driver.find_element(*locator))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 162, in _element_if_visible
    return element if element.is_displayed() == visibility else False
                      ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'is_displayed'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 81, in get_company_finance_data
    company_name, company_MDN = self.get_company_name_MDN()
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 125, in get_company_name_MDN
    if self.driver.find_element(By.ID, 'pt2:t2::emptyTxt'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000108f98428 chromedriver + 4899880
1   chromedriver                        0x0000000108f15a23 chromedriver + 4364835
2   chromedriver                        0x0000000108b5fbf6 chromedriver + 474102
3   chromedriver                        0x0000000108b35cdc chromedriver + 302300
4   chromedriver                        0x0000000108bce85f chromedriver + 927839
5   chromedriver                        0x0000000108be4226 chromedriver + 1016358
6   chromedriver                        0x0000000108bc9333 chromedriver + 906035
7   chromedriver                        0x0000000108b9355f chromedriver + 685407
8   chromedriver                        0x0000000108b94a7e chromedriver + 690814
9   chromedriver                        0x0000000108f6579e chromedriver + 4691870
10  chromedriver                        0x0000000108f6a961 chromedriver + 4712801
11  chromedriver                        0x0000000108f712ff chromedriver + 4739839
12  chromedriver                        0x0000000108f6b85a chromedriver + 4716634
13  chromedriver                        0x0000000108f3dfce chromedriver + 4530126
14  chromedriver                        0x0000000108f8b5c8 chromedriver + 4847048
15  chromedriver                        0x0000000108f8b747 chromedriver + 4847431
16  chromedriver                        0x0000000108fa087f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000108f98428 chromedriver + 4899880
1   chromedriver                        0x0000000108f15a23 chromedriver + 4364835
2   chromedriver                        0x0000000108b5fbf6 chromedriver + 474102
3   chromedriver                        0x0000000108b35cdc chromedriver + 302300
4   chromedriver                        0x0000000108bce85f chromedriver + 927839
5   chromedriver                        0x0000000108be4226 chromedriver + 1016358
6   chromedriver                        0x0000000108bc9333 chromedriver + 906035
7   chromedriver                        0x0000000108b9355f chromedriver + 685407
8   chromedriver                        0x0000000108b94a7e chromedriver + 690814
9   chromedriver                        0x0000000108f6579e chromedriver + 4691870
10  chromedriver                        0x0000000108f6a961 chromedriver + 4712801
11  chromedriver                        0x0000000108f712ff chromedriver + 4739839
12  chromedriver                        0x0000000108f6b85a chromedriver + 4716634
13  chromedriver                        0x0000000108f3dfce chromedriver + 4530126
14  chromedriver                        0x0000000108f8b5c8 chromedriver + 4847048
15  chromedriver                        0x0000000108f8b747 chromedriver + 4847431
16  chromedriver                        0x0000000108fa087f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:24:51 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:24:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 646,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 80636,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 419.527285,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 24, 51, 528701),
 'httpcompression/response_bytes': 257180,
 'httpcompression/response_count': 2,
 'item_scraped_count': 7,
 'log_count/ERROR': 1,
 'log_count/INFO': 22,
 'log_count/WARNING': 3,
 'memusage/max': 134287360,
 'memusage/startup': 129884160,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 17, 52, 1416)}
2023-03-28 14:24:51 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:24:52 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:24:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 87, in get_company_finance_data
    self.get_report_data(year=year, quarter=quarter, report_name = report_name, company_name=company_name, company_MDN=company_MDN)
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 158, in get_report_data
    economic_infor_report_data = self.wait.until(EC.visibility_of_element_located((By.ID, f'pt2:tab{tab_num}::body'))).get_attribute('innerHTML')
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 139, in _predicate
    return _element_if_visible(driver.find_element(*locator))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010687e428 chromedriver + 4899880
1   chromedriver                        0x00000001067fba23 chromedriver + 4364835
2   chromedriver                        0x0000000106445bf6 chromedriver + 474102
3   chromedriver                        0x000000010641bcdc chromedriver + 302300
4   chromedriver                        0x00000001064b485f chromedriver + 927839
5   chromedriver                        0x00000001064ca226 chromedriver + 1016358
6   chromedriver                        0x00000001064af333 chromedriver + 906035
7   chromedriver                        0x000000010647955f chromedriver + 685407
8   chromedriver                        0x000000010647aa7e chromedriver + 690814
9   chromedriver                        0x000000010684b79e chromedriver + 4691870
10  chromedriver                        0x0000000106850961 chromedriver + 4712801
11  chromedriver                        0x00000001068572ff chromedriver + 4739839
12  chromedriver                        0x000000010685185a chromedriver + 4716634
13  chromedriver                        0x0000000106823fce chromedriver + 4530126
14  chromedriver                        0x00000001068715c8 chromedriver + 4847048
15  chromedriver                        0x0000000106871747 chromedriver + 4847431
16  chromedriver                        0x000000010688687f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010687e428 chromedriver + 4899880
1   chromedriver                        0x00000001067fba23 chromedriver + 4364835
2   chromedriver                        0x0000000106445bf6 chromedriver + 474102
3   chromedriver                        0x000000010641bcdc chromedriver + 302300
4   chromedriver                        0x00000001064b485f chromedriver + 927839
5   chromedriver                        0x00000001064ca226 chromedriver + 1016358
6   chromedriver                        0x00000001064af333 chromedriver + 906035
7   chromedriver                        0x000000010647955f chromedriver + 685407
8   chromedriver                        0x000000010647aa7e chromedriver + 690814
9   chromedriver                        0x000000010684b79e chromedriver + 4691870
10  chromedriver                        0x0000000106850961 chromedriver + 4712801
11  chromedriver                        0x00000001068572ff chromedriver + 4739839
12  chromedriver                        0x000000010685185a chromedriver + 4716634
13  chromedriver                        0x0000000106823fce chromedriver + 4530126
14  chromedriver                        0x00000001068715c8 chromedriver + 4847048
15  chromedriver                        0x0000000106871747 chromedriver + 4847431
16  chromedriver                        0x000000010688687f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:24:52 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 8 items (at 0 items/min)
2023-03-28 14:24:52 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:24:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 489,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 79448,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 420.08092,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 24, 52, 86615),
 'httpcompression/response_bytes': 250504,
 'httpcompression/response_count': 2,
 'item_scraped_count': 8,
 'log_count/ERROR': 1,
 'log_count/INFO': 23,
 'log_count/WARNING': 3,
 'memusage/max': 143872000,
 'memusage/startup': 130240512,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 17, 52, 5695)}
2023-03-28 14:24:52 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:24:55 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:24:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 74, in get_company_finance_data
    self.wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'xgl')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 139, in _predicate
    return _element_if_visible(driver.find_element(*locator))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010a610428 chromedriver + 4899880
1   chromedriver                        0x000000010a58da23 chromedriver + 4364835
2   chromedriver                        0x000000010a1d7bf6 chromedriver + 474102
3   chromedriver                        0x000000010a1adcdc chromedriver + 302300
4   chromedriver                        0x000000010a24685f chromedriver + 927839
5   chromedriver                        0x000000010a25c226 chromedriver + 1016358
6   chromedriver                        0x000000010a241333 chromedriver + 906035
7   chromedriver                        0x000000010a20b55f chromedriver + 685407
8   chromedriver                        0x000000010a20ca7e chromedriver + 690814
9   chromedriver                        0x000000010a5dd79e chromedriver + 4691870
10  chromedriver                        0x000000010a5e2961 chromedriver + 4712801
11  chromedriver                        0x000000010a5e92ff chromedriver + 4739839
12  chromedriver                        0x000000010a5e385a chromedriver + 4716634
13  chromedriver                        0x000000010a5b5fce chromedriver + 4530126
14  chromedriver                        0x000000010a6035c8 chromedriver + 4847048
15  chromedriver                        0x000000010a603747 chromedriver + 4847431
16  chromedriver                        0x000000010a61887f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010a610428 chromedriver + 4899880
1   chromedriver                        0x000000010a58da23 chromedriver + 4364835
2   chromedriver                        0x000000010a1d7bf6 chromedriver + 474102
3   chromedriver                        0x000000010a1adcdc chromedriver + 302300
4   chromedriver                        0x000000010a24685f chromedriver + 927839
5   chromedriver                        0x000000010a25c226 chromedriver + 1016358
6   chromedriver                        0x000000010a241333 chromedriver + 906035
7   chromedriver                        0x000000010a20b55f chromedriver + 685407
8   chromedriver                        0x000000010a20ca7e chromedriver + 690814
9   chromedriver                        0x000000010a5dd79e chromedriver + 4691870
10  chromedriver                        0x000000010a5e2961 chromedriver + 4712801
11  chromedriver                        0x000000010a5e92ff chromedriver + 4739839
12  chromedriver                        0x000000010a5e385a chromedriver + 4716634
13  chromedriver                        0x000000010a5b5fce chromedriver + 4530126
14  chromedriver                        0x000000010a6035c8 chromedriver + 4847048
15  chromedriver                        0x000000010a603747 chromedriver + 4847431
16  chromedriver                        0x000000010a61887f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:24:55 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 6 items (at 1 items/min)
2023-03-28 14:24:55 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:24:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 669,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 85348,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 423.45355,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 24, 55, 448255),
 'httpcompression/response_bytes': 269004,
 'httpcompression/response_count': 2,
 'item_scraped_count': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 21,
 'log_count/WARNING': 4,
 'memusage/max': 130195456,
 'memusage/startup': 130195456,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 17, 51, 994705)}
2023-03-28 14:24:55 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:28:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:28:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:28:36 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-28 14:28:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:28:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:28:36 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-28 14:28:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:28:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:28:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-28 14:28:36 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:28:36 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:28:36 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-28 14:28:36 [scrapy.extensions.telnet] INFO: Telnet Password: 162555317f3a2db2
2023-03-28 14:28:36 [scrapy.extensions.telnet] INFO: Telnet Password: d73fd7d0f3ede97b
2023-03-28 14:28:36 [scrapy.extensions.telnet] INFO: Telnet Password: 3aabd5ede789ce18
2023-03-28 14:28:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:28:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:28:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2023-03-28 14:28:36 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:28:36 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:28:36 [WDM] INFO: ====== WebDriver manager ======
2023-03-28 14:28:36 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:28:36 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:28:36 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-28 14:28:36 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:28:36 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:28:36 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-28 14:28:40 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:28:40 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:28:40 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:28:40 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:28:40 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-28 14:28:40 [scrapy.middleware] INFO: Enabled item pipelines:
['Spider_POC_SSC.pipelines.BigQueryPipeline']
2023-03-28 14:28:40 [scrapy.core.engine] INFO: Spider opened
2023-03-28 14:28:41 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:28:42 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:28:42 [root] INFO: Dataset spider-poc.spider_poc_ssc already exist
2023-03-28 14:28:47 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:28:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:28:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-28 14:28:47 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:28:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:28:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-28 14:28:47 [root] INFO: Table spider-poc.spider_poc_ssc.company_finance_0328 already exist
2023-03-28 14:28:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-28 14:28:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-28 14:29:12 [root] INFO: First page:1264, Max page:2526
2023-03-28 14:29:27 [root] INFO: First page:1, Max page:1263
2023-03-28 14:29:30 [root] INFO: First page:2527, Max page:3791
2023-03-28 14:29:56 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:30:02 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 2 items (at 2 items/min)
2023-03-28 14:30:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 14:30:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2023-03-28 14:31:05 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 2 items/min)
2023-03-28 14:31:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2023-03-28 14:31:59 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:32:15 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:32:30 [root] WARNING: Error occurred: Search Wrong Retry Times#2
2023-03-28 14:32:54 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2023-03-28 14:33:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2023-03-28 14:33:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 6 items (at 2 items/min)
2023-03-28 14:33:53 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:33:56 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 7 items (at 1 items/min)
2023-03-28 14:33:59 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:34:32 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 2 items/min)
2023-03-28 14:35:00 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 9 items (at 2 items/min)
2023-03-28 14:35:05 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 1 items/min)
2023-03-28 14:35:35 [root] WARNING: Error occurred: Search Wrong Retry Times#2
2023-03-28 14:36:07 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 11 items (at 2 items/min)
2023-03-28 14:36:12 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 1 items/min)
2023-03-28 14:36:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 7 items (at 2 items/min)
2023-03-28 14:36:44 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:36:47 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 8 items (at 1 items/min)
2023-03-28 14:37:16 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 13 items (at 2 items/min)
2023-03-28 14:37:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 5 items (at 1 items/min)
2023-03-28 14:37:33 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:37:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 71, in get_company_finance_data
    self.wait_until_done()
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 105, in wait_until_done
    self.wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 69, in _predicate
    return driver.find_element(*locator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000109236428 chromedriver + 4899880
1   chromedriver                        0x00000001091b3a23 chromedriver + 4364835
2   chromedriver                        0x0000000108dfdbf6 chromedriver + 474102
3   chromedriver                        0x0000000108dd3cdc chromedriver + 302300
4   chromedriver                        0x0000000108e6c85f chromedriver + 927839
5   chromedriver                        0x0000000108e82226 chromedriver + 1016358
6   chromedriver                        0x0000000108e67333 chromedriver + 906035
7   chromedriver                        0x0000000108e3155f chromedriver + 685407
8   chromedriver                        0x0000000108e32a7e chromedriver + 690814
9   chromedriver                        0x000000010920379e chromedriver + 4691870
10  chromedriver                        0x0000000109208961 chromedriver + 4712801
11  chromedriver                        0x000000010920f2ff chromedriver + 4739839
12  chromedriver                        0x000000010920985a chromedriver + 4716634
13  chromedriver                        0x00000001091dbfce chromedriver + 4530126
14  chromedriver                        0x00000001092295c8 chromedriver + 4847048
15  chromedriver                        0x0000000109229747 chromedriver + 4847431
16  chromedriver                        0x000000010923e87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x0000000109236428 chromedriver + 4899880
1   chromedriver                        0x00000001091b3a23 chromedriver + 4364835
2   chromedriver                        0x0000000108dfdbf6 chromedriver + 474102
3   chromedriver                        0x0000000108dd3cdc chromedriver + 302300
4   chromedriver                        0x0000000108e6c85f chromedriver + 927839
5   chromedriver                        0x0000000108e82226 chromedriver + 1016358
6   chromedriver                        0x0000000108e67333 chromedriver + 906035
7   chromedriver                        0x0000000108e3155f chromedriver + 685407
8   chromedriver                        0x0000000108e32a7e chromedriver + 690814
9   chromedriver                        0x000000010920379e chromedriver + 4691870
10  chromedriver                        0x0000000109208961 chromedriver + 4712801
11  chromedriver                        0x000000010920f2ff chromedriver + 4739839
12  chromedriver                        0x000000010920985a chromedriver + 4716634
13  chromedriver                        0x00000001091dbfce chromedriver + 4530126
14  chromedriver                        0x00000001092295c8 chromedriver + 4847048
15  chromedriver                        0x0000000109229747 chromedriver + 4847431
16  chromedriver                        0x000000010923e87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:37:33 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:37:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 609,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 81524,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 526.394923,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 37, 33, 857444),
 'httpcompression/response_bytes': 259443,
 'httpcompression/response_count': 2,
 'item_scraped_count': 5,
 'log_count/ERROR': 1,
 'log_count/INFO': 21,
 'log_count/WARNING': 9,
 'memusage/max': 144195584,
 'memusage/startup': 130281472,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 28, 47, 462521)}
2023-03-28 14:37:33 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:37:35 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:37:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 71, in get_company_finance_data
    self.wait_until_done()
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 105, in wait_until_done
    self.wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 69, in _predicate
    return driver.find_element(*locator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010ad10428 chromedriver + 4899880
1   chromedriver                        0x000000010ac8da23 chromedriver + 4364835
2   chromedriver                        0x000000010a8d7bf6 chromedriver + 474102
3   chromedriver                        0x000000010a8adcdc chromedriver + 302300
4   chromedriver                        0x000000010a94685f chromedriver + 927839
5   chromedriver                        0x000000010a95c226 chromedriver + 1016358
6   chromedriver                        0x000000010a941333 chromedriver + 906035
7   chromedriver                        0x000000010a90b55f chromedriver + 685407
8   chromedriver                        0x000000010a90ca7e chromedriver + 690814
9   chromedriver                        0x000000010acdd79e chromedriver + 4691870
10  chromedriver                        0x000000010ace2961 chromedriver + 4712801
11  chromedriver                        0x000000010ace92ff chromedriver + 4739839
12  chromedriver                        0x000000010ace385a chromedriver + 4716634
13  chromedriver                        0x000000010acb5fce chromedriver + 4530126
14  chromedriver                        0x000000010ad035c8 chromedriver + 4847048
15  chromedriver                        0x000000010ad03747 chromedriver + 4847431
16  chromedriver                        0x000000010ad1887f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010ad10428 chromedriver + 4899880
1   chromedriver                        0x000000010ac8da23 chromedriver + 4364835
2   chromedriver                        0x000000010a8d7bf6 chromedriver + 474102
3   chromedriver                        0x000000010a8adcdc chromedriver + 302300
4   chromedriver                        0x000000010a94685f chromedriver + 927839
5   chromedriver                        0x000000010a95c226 chromedriver + 1016358
6   chromedriver                        0x000000010a941333 chromedriver + 906035
7   chromedriver                        0x000000010a90b55f chromedriver + 685407
8   chromedriver                        0x000000010a90ca7e chromedriver + 690814
9   chromedriver                        0x000000010acdd79e chromedriver + 4691870
10  chromedriver                        0x000000010ace2961 chromedriver + 4712801
11  chromedriver                        0x000000010ace92ff chromedriver + 4739839
12  chromedriver                        0x000000010ace385a chromedriver + 4716634
13  chromedriver                        0x000000010acb5fce chromedriver + 4530126
14  chromedriver                        0x000000010ad035c8 chromedriver + 4847048
15  chromedriver                        0x000000010ad03747 chromedriver + 4847431
16  chromedriver                        0x000000010ad1887f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:37:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:37:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 691,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 84217,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 528.105904,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 37, 35, 253141),
 'httpcompression/response_bytes': 264903,
 'httpcompression/response_count': 2,
 'item_scraped_count': 9,
 'log_count/ERROR': 1,
 'log_count/INFO': 22,
 'log_count/WARNING': 5,
 'memusage/max': 148013056,
 'memusage/startup': 129597440,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 28, 47, 147237)}
2023-03-28 14:37:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-28 14:37:38 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-28 14:37:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 74, in get_company_finance_data
    self.wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'xgl')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 86, in until
    value = method(self._driver)
            ^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/expected_conditions.py", line 139, in _predicate
    return _element_if_visible(driver.find_element(*locator))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 830, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010a123428 chromedriver + 4899880
1   chromedriver                        0x000000010a0a0a23 chromedriver + 4364835
2   chromedriver                        0x0000000109ceabf6 chromedriver + 474102
3   chromedriver                        0x0000000109cc0cdc chromedriver + 302300
4   chromedriver                        0x0000000109d5985f chromedriver + 927839
5   chromedriver                        0x0000000109d6f226 chromedriver + 1016358
6   chromedriver                        0x0000000109d54333 chromedriver + 906035
7   chromedriver                        0x0000000109d1e55f chromedriver + 685407
8   chromedriver                        0x0000000109d1fa7e chromedriver + 690814
9   chromedriver                        0x000000010a0f079e chromedriver + 4691870
10  chromedriver                        0x000000010a0f5961 chromedriver + 4712801
11  chromedriver                        0x000000010a0fc2ff chromedriver + 4739839
12  chromedriver                        0x000000010a0f685a chromedriver + 4716634
13  chromedriver                        0x000000010a0c8fce chromedriver + 4530126
14  chromedriver                        0x000000010a1165c8 chromedriver + 4847048
15  chromedriver                        0x000000010a116747 chromedriver + 4847431
16  chromedriver                        0x000000010a12b87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 93, in get_company_finance_data
    self.driver.get("https://congbothongtin.ssc.gov.vn/faces/NewsSearch")
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 449, in get
    self.execute(Command.GET, {"url": url})
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=111.0.5563.110)
Stacktrace:
0   chromedriver                        0x000000010a123428 chromedriver + 4899880
1   chromedriver                        0x000000010a0a0a23 chromedriver + 4364835
2   chromedriver                        0x0000000109ceabf6 chromedriver + 474102
3   chromedriver                        0x0000000109cc0cdc chromedriver + 302300
4   chromedriver                        0x0000000109d5985f chromedriver + 927839
5   chromedriver                        0x0000000109d6f226 chromedriver + 1016358
6   chromedriver                        0x0000000109d54333 chromedriver + 906035
7   chromedriver                        0x0000000109d1e55f chromedriver + 685407
8   chromedriver                        0x0000000109d1fa7e chromedriver + 690814
9   chromedriver                        0x000000010a0f079e chromedriver + 4691870
10  chromedriver                        0x000000010a0f5961 chromedriver + 4712801
11  chromedriver                        0x000000010a0fc2ff chromedriver + 4739839
12  chromedriver                        0x000000010a0f685a chromedriver + 4716634
13  chromedriver                        0x000000010a0c8fce chromedriver + 4530126
14  chromedriver                        0x000000010a1165c8 chromedriver + 4847048
15  chromedriver                        0x000000010a116747 chromedriver + 4847431
16  chromedriver                        0x000000010a12b87f chromedriver + 4933759
17  libsystem_pthread.dylib             0x00007ff816cc2259 _pthread_start + 125
18  libsystem_pthread.dylib             0x00007ff816cbdc7b thread_start + 15

2023-03-28 14:37:38 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-28 14:37:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 609,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 85341,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 531.264568,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 28, 6, 37, 38, 732552),
 'httpcompression/response_bytes': 269024,
 'httpcompression/response_count': 2,
 'item_scraped_count': 13,
 'log_count/ERROR': 1,
 'log_count/INFO': 23,
 'log_count/WARNING': 3,
 'memusage/max': 146247680,
 'memusage/startup': 130310144,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchWindowException': 1,
 'start_time': datetime.datetime(2023, 3, 28, 6, 28, 47, 467984)}
2023-03-28 14:37:38 [scrapy.core.engine] INFO: Spider closed (finished)
