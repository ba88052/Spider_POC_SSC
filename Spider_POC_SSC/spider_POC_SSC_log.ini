2023-03-13 00:40:00 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-13 00:40:00 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-13 00:40:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-13 00:40:00 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-13 00:40:00 [scrapy.extensions.telnet] INFO: Telnet Password: bb7ef632c7fe1d38
2023-03-13 00:40:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-13 00:40:00 [WDM] INFO: ====== WebDriver manager ======
2023-03-13 00:40:00 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-13 00:40:01 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-13 00:40:02 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-13 00:40:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-13 00:40:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-13 00:40:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-13 00:40:02 [scrapy.core.engine] INFO: Spider opened
2023-03-13 00:40:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-13 00:40:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-13 00:40:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_info.py", line 91, in parse
    SPIDE_RPOC_SSC_COMPANY_INFO_ITEM["COMPANY_GENERAL_INFO"] = self.get_first_data(company_name).to_json()
                                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_info.py", line 130, in get_first_data
    os.rename(f"{self.current_dir}/ThongTinHoSo.xls", f"{self.download_directory}/{company_name}_general_info.xls")
FileNotFoundError: [Errno 2] No such file or directory: '/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/ThongTinHoSo.xls' -> '/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/SSC_Company_Data/Tổng CT Công nghiệp mỏ Việt Bắc TKV-CTCP_general_info.xls'
2023-03-13 00:40:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-13 00:40:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 583,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 78902,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 33.672561,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 12, 16, 40, 36, 301007),
 'httpcompression/response_bytes': 249672,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'log_count/WARNING': 2,
 'memusage/max': 108929024,
 'memusage/startup': 108929024,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/FileNotFoundError': 1,
 'start_time': datetime.datetime(2023, 3, 12, 16, 40, 2, 628446)}
2023-03-13 00:40:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-13 00:44:04 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-13 00:44:04 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-13 00:44:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-13 00:44:04 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-13 00:44:04 [scrapy.extensions.telnet] INFO: Telnet Password: 294d863e4b2654d0
2023-03-13 00:44:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-13 00:44:04 [WDM] INFO: ====== WebDriver manager ======
2023-03-13 00:44:04 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-13 00:44:05 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-13 00:44:06 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-13 00:44:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-13 00:44:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-13 00:44:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-13 00:44:06 [scrapy.core.engine] INFO: Spider opened
2023-03-13 00:44:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-13 00:44:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-13 00:44:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_info.py", line 91, in parse
    SPIDE_RPOC_SSC_COMPANY_INFO_ITEM["COMPANY_GENERAL_INFO"] = self.get_first_data(company_name).to_json()
                                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_info.py", line 132, in get_first_data
    os.rename(f"{self.current_dir}/ThongTinHoSo.xls", f"{self.download_directory}/{company_name}_general_info.xls")
FileNotFoundError: [Errno 2] No such file or directory: '/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/ThongTinHoSo.xls' -> '/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/SSC_Company_Data/Tổng CT Công nghiệp mỏ Việt Bắc TKV-CTCP_general_info.xls'
2023-03-13 00:44:35 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-13 00:44:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 656,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 86150,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 28.451101,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 12, 16, 44, 35, 334350),
 'httpcompression/response_bytes': 270160,
 'httpcompression/response_count': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'log_count/WARNING': 2,
 'memusage/max': 108949504,
 'memusage/startup': 108949504,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/FileNotFoundError': 1,
 'start_time': datetime.datetime(2023, 3, 12, 16, 44, 6, 883249)}
2023-03-13 00:44:35 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-13 00:46:54 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-13 00:46:54 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-13 00:46:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-13 00:46:54 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-13 00:46:54 [scrapy.extensions.telnet] INFO: Telnet Password: 67f1b81c92c320a0
2023-03-13 00:46:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-13 00:46:54 [WDM] INFO: ====== WebDriver manager ======
2023-03-13 00:46:54 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-13 00:46:55 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-13 00:46:56 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-13 00:46:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-13 00:46:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-13 00:46:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-13 00:46:56 [scrapy.core.engine] INFO: Spider opened
2023-03-13 00:46:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-13 00:46:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2023-03-13 00:49:08 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-13 00:49:08 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-13 00:49:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-13 00:49:08 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-13 00:49:08 [scrapy.extensions.telnet] INFO: Telnet Password: a7e05cdd70eaea05
2023-03-13 00:49:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-13 00:49:08 [WDM] INFO: ====== WebDriver manager ======
2023-03-13 00:49:09 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-13 00:49:09 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-13 00:49:10 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-13 00:49:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-13 00:49:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-13 00:49:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-13 00:49:10 [scrapy.core.engine] INFO: Spider opened
2023-03-13 00:49:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-13 00:49:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-13 00:50:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 2 items (at 2 items/min)
2023-03-13 00:51:17 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 2 items/min)
2023-03-13 00:52:37 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 7 items (at 3 items/min)
2023-03-13 00:53:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 9 items (at 2 items/min)
2023-03-13 00:54:16 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 11 items (at 2 items/min)
2023-03-13 00:55:32 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 14 items (at 3 items/min)
2023-03-13 00:56:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 16 items (at 2 items/min)
2023-03-13 00:57:13 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 18 items (at 2 items/min)
2023-03-13 00:58:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 21 items (at 3 items/min)
2023-03-13 00:59:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 23 items (at 2 items/min)
2023-03-13 01:00:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 25 items (at 2 items/min)
2023-03-13 01:01:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 27 items (at 2 items/min)
2023-03-13 01:02:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 29 items (at 2 items/min)
2023-03-13 01:03:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 32 items (at 3 items/min)
2023-03-13 01:04:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 34 items (at 2 items/min)
2023-03-13 01:05:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 36 items (at 2 items/min)
2023-03-13 01:06:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 38 items (at 2 items/min)
2023-03-13 01:07:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 40 items (at 2 items/min)
2023-03-13 01:08:13 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 42 items (at 2 items/min)
2023-03-13 01:09:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 45 items (at 3 items/min)
2023-03-13 01:10:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 47 items (at 2 items/min)
2023-03-13 01:11:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 49 items (at 2 items/min)
2023-03-13 01:12:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 51 items (at 2 items/min)
2023-03-13 01:13:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 53 items (at 2 items/min)
2023-03-13 01:14:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 55 items (at 2 items/min)
2023-03-13 01:15:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 57 items (at 2 items/min)
2023-03-13 01:16:16 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 59 items (at 2 items/min)
2023-03-13 01:17:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 61 items (at 2 items/min)
2023-03-13 01:18:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 63 items (at 2 items/min)
2023-03-13 01:19:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 65 items (at 2 items/min)
2023-03-13 01:20:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 67 items (at 2 items/min)
2023-03-13 01:21:37 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 70 items (at 3 items/min)
2023-03-13 01:22:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 72 items (at 2 items/min)
2023-03-13 01:23:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 74 items (at 2 items/min)
2023-03-13 01:24:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 76 items (at 2 items/min)
2023-03-13 01:25:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 78 items (at 2 items/min)
2023-03-13 01:26:13 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 80 items (at 2 items/min)
2023-03-13 01:27:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 83 items (at 3 items/min)
2023-03-13 01:28:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 85 items (at 2 items/min)
2023-03-13 01:29:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 87 items (at 2 items/min)
2023-03-13 01:30:14 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 89 items (at 2 items/min)
2023-03-13 01:31:39 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 92 items (at 3 items/min)
2023-03-13 01:32:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 94 items (at 2 items/min)
2023-03-13 01:33:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 96 items (at 2 items/min)
2023-03-13 01:34:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 98 items (at 2 items/min)
2023-03-13 01:35:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 100 items (at 2 items/min)
2023-03-13 01:36:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 102 items (at 2 items/min)
2023-03-13 01:37:38 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 105 items (at 3 items/min)
2023-03-13 01:38:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 107 items (at 2 items/min)
2023-03-13 01:39:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 109 items (at 2 items/min)
2023-03-13 01:40:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 111 items (at 2 items/min)
2023-03-13 01:41:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 113 items (at 2 items/min)
2023-03-13 01:42:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 115 items (at 2 items/min)
2023-03-13 01:43:17 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 117 items (at 2 items/min)
2023-03-13 01:44:13 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 119 items (at 2 items/min)
2023-03-13 01:45:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 122 items (at 3 items/min)
2023-03-13 01:46:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 124 items (at 2 items/min)
2023-03-13 01:47:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 126 items (at 2 items/min)
2023-03-13 01:48:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 128 items (at 2 items/min)
2023-03-13 01:49:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 130 items (at 2 items/min)
2023-03-13 01:50:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 132 items (at 2 items/min)
2023-03-13 01:51:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 134 items (at 2 items/min)
2023-03-13 01:52:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 137 items (at 3 items/min)
2023-03-13 01:53:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 139 items (at 2 items/min)
2023-03-13 01:54:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 141 items (at 2 items/min)
2023-03-13 01:55:14 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 143 items (at 2 items/min)
2023-03-13 01:56:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 146 items (at 3 items/min)
2023-03-13 01:57:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 148 items (at 2 items/min)
2023-03-13 01:58:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 150 items (at 2 items/min)
2023-03-13 01:59:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 152 items (at 2 items/min)
2023-03-13 02:00:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 154 items (at 2 items/min)
2023-03-13 02:01:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 156 items (at 2 items/min)
2023-03-13 02:02:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 158 items (at 2 items/min)
2023-03-13 02:03:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 161 items (at 3 items/min)
2023-03-13 02:04:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 163 items (at 2 items/min)
2023-03-13 02:05:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 165 items (at 2 items/min)
2023-03-13 02:06:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 167 items (at 2 items/min)
2023-03-13 02:07:12 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 169 items (at 2 items/min)
2023-03-13 02:08:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 172 items (at 3 items/min)
2023-03-13 02:09:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 174 items (at 2 items/min)
2023-03-13 02:10:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 176 items (at 2 items/min)
2023-03-13 02:11:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 178 items (at 2 items/min)
2023-03-13 02:12:12 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 180 items (at 2 items/min)
2023-03-13 02:13:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 183 items (at 3 items/min)
2023-03-13 02:14:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 185 items (at 2 items/min)
2023-03-13 02:15:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 187 items (at 2 items/min)
2023-03-13 02:16:21 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 189 items (at 2 items/min)
2023-03-13 02:17:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 191 items (at 2 items/min)
2023-03-13 02:18:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 193 items (at 2 items/min)
2023-03-13 02:19:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 195 items (at 2 items/min)
2023-03-13 02:20:12 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 197 items (at 2 items/min)
2023-03-13 02:21:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 200 items (at 3 items/min)
2023-03-13 02:22:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 202 items (at 2 items/min)
2023-03-13 02:23:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 204 items (at 2 items/min)
2023-03-13 02:24:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 206 items (at 2 items/min)
2023-03-13 02:25:39 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 209 items (at 3 items/min)
2023-03-13 02:26:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 211 items (at 2 items/min)
2023-03-13 02:27:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 213 items (at 2 items/min)
2023-03-13 02:28:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 215 items (at 2 items/min)
2023-03-13 02:29:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 217 items (at 2 items/min)
2023-03-13 02:30:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 219 items (at 2 items/min)
2023-03-13 02:31:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 221 items (at 2 items/min)
2023-03-13 02:32:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 223 items (at 2 items/min)
2023-03-13 02:33:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 226 items (at 3 items/min)
2023-03-13 02:34:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 228 items (at 2 items/min)
2023-03-13 02:35:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 230 items (at 2 items/min)
2023-03-13 02:36:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 232 items (at 2 items/min)
2023-03-13 02:37:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 234 items (at 2 items/min)
2023-03-13 02:38:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 236 items (at 2 items/min)
2023-03-13 02:39:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 238 items (at 2 items/min)
2023-03-13 02:40:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 241 items (at 3 items/min)
2023-03-13 02:41:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 243 items (at 2 items/min)
2023-03-13 02:42:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 245 items (at 2 items/min)
2023-03-13 02:43:17 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 247 items (at 2 items/min)
2023-03-13 02:44:12 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 249 items (at 2 items/min)
2023-03-13 02:45:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 252 items (at 3 items/min)
2023-03-13 02:46:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 254 items (at 2 items/min)
2023-03-13 02:47:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 256 items (at 2 items/min)
2023-03-13 02:48:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 258 items (at 2 items/min)
2023-03-13 02:49:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 260 items (at 2 items/min)
2023-03-13 02:50:17 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 262 items (at 2 items/min)
2023-03-13 02:51:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 264 items (at 2 items/min)
2023-03-13 02:52:37 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 267 items (at 3 items/min)
2023-03-13 02:53:32 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 269 items (at 2 items/min)
2023-03-13 02:54:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 271 items (at 2 items/min)
2023-03-13 02:55:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 273 items (at 2 items/min)
2023-03-13 02:56:14 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 275 items (at 2 items/min)
2023-03-13 02:57:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 278 items (at 3 items/min)
2023-03-13 02:58:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 280 items (at 2 items/min)
2023-03-13 02:59:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 282 items (at 2 items/min)
2023-03-13 03:00:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 284 items (at 2 items/min)
2023-03-13 03:01:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 286 items (at 2 items/min)
2023-03-13 03:02:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 289 items (at 3 items/min)
2023-03-13 03:03:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 291 items (at 2 items/min)
2023-03-13 03:04:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 293 items (at 2 items/min)
2023-03-13 03:05:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 295 items (at 2 items/min)
2023-03-13 03:06:14 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 297 items (at 2 items/min)
2023-03-13 03:07:37 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 300 items (at 3 items/min)
2023-03-13 03:08:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 302 items (at 2 items/min)
2023-03-13 03:09:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 304 items (at 2 items/min)
2023-03-13 03:10:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 306 items (at 2 items/min)
2023-03-13 03:11:21 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 308 items (at 2 items/min)
2023-03-13 03:12:16 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 310 items (at 2 items/min)
2023-03-13 03:13:37 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 313 items (at 3 items/min)
2023-03-13 03:14:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 315 items (at 2 items/min)
2023-03-13 03:15:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 317 items (at 2 items/min)
2023-03-13 03:16:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 319 items (at 2 items/min)
2023-03-13 03:17:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 321 items (at 2 items/min)
2023-03-13 03:18:13 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 323 items (at 2 items/min)
2023-03-13 03:19:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 325 items (at 2 items/min)
2023-03-13 03:20:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 327 items (at 2 items/min)
2023-03-13 03:21:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 330 items (at 3 items/min)
2023-03-13 03:22:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 332 items (at 2 items/min)
2023-03-13 03:23:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 334 items (at 2 items/min)
2023-03-13 03:24:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 336 items (at 2 items/min)
2023-03-13 03:25:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 338 items (at 2 items/min)
2023-03-13 03:26:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 340 items (at 2 items/min)
2023-03-13 03:27:12 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 342 items (at 2 items/min)
2023-03-13 03:28:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 345 items (at 3 items/min)
2023-03-13 03:29:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 347 items (at 2 items/min)
2023-03-13 03:30:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 349 items (at 2 items/min)
2023-03-13 03:31:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 351 items (at 2 items/min)
2023-03-13 03:32:16 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 353 items (at 2 items/min)
2023-03-13 03:33:14 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 355 items (at 2 items/min)
2023-03-13 03:34:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 358 items (at 3 items/min)
2023-03-13 03:35:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 360 items (at 2 items/min)
2023-03-13 03:36:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 362 items (at 2 items/min)
2023-03-13 03:37:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 364 items (at 2 items/min)
2023-03-13 03:38:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 366 items (at 2 items/min)
2023-03-13 03:39:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 368 items (at 2 items/min)
2023-03-13 03:40:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 370 items (at 2 items/min)
2023-03-13 03:41:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 372 items (at 2 items/min)
2023-03-13 03:42:16 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 373 items (at 1 items/min)
2023-03-13 03:43:14 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 375 items (at 2 items/min)
2023-03-13 03:44:11 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 377 items (at 2 items/min)
2023-03-13 03:45:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 380 items (at 3 items/min)
2023-03-13 03:46:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 382 items (at 2 items/min)
2023-03-13 03:47:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 384 items (at 2 items/min)
2023-03-13 03:48:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 386 items (at 2 items/min)
2023-03-13 03:49:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 387 items (at 1 items/min)
2023-03-13 03:50:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 389 items (at 2 items/min)
2023-03-13 03:51:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 390 items (at 1 items/min)
2023-03-13 03:52:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 392 items (at 2 items/min)
2023-03-13 03:53:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 393 items (at 1 items/min)
2023-03-13 03:54:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 395 items (at 2 items/min)
2023-03-13 03:55:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 396 items (at 1 items/min)
2023-03-13 03:56:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 398 items (at 2 items/min)
2023-03-13 03:57:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 400 items (at 2 items/min)
2023-03-13 03:58:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 402 items (at 2 items/min)
2023-03-13 03:59:51 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 403 items (at 1 items/min)
2023-03-13 04:00:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 404 items (at 1 items/min)
2023-03-13 04:01:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 406 items (at 2 items/min)
2023-03-13 04:02:39 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 408 items (at 2 items/min)
2023-03-13 04:03:52 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 410 items (at 2 items/min)
2023-03-13 04:04:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 411 items (at 1 items/min)
2023-03-13 04:06:12 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 413 items (at 2 items/min)
2023-03-13 04:07:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 414 items (at 1 items/min)
2023-03-13 04:08:50 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 416 items (at 2 items/min)
2023-03-13 04:09:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 417 items (at 1 items/min)
2023-03-13 04:10:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 419 items (at 2 items/min)
2023-03-13 04:11:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 421 items (at 2 items/min)
2023-03-13 04:12:17 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 422 items (at 1 items/min)
2023-03-13 04:13:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 423 items (at 1 items/min)
2023-03-13 04:14:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 425 items (at 2 items/min)
2023-03-13 04:15:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 427 items (at 2 items/min)
2023-03-13 04:16:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 429 items (at 2 items/min)
2023-03-13 04:17:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 430 items (at 1 items/min)
2023-03-13 04:18:21 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 432 items (at 2 items/min)
2023-03-13 04:18:55 [root] WARNING: Error occurred: Search Wrong Retry Times#1
2023-03-13 04:19:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 433 items (at 1 items/min)
2023-03-13 04:25:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_info.py", line 101, in parse
    self.driver.back()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 643, in back
    self.execute(Command.GO_BACK)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 440, in execute
    self.error_handler.check_response(response)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 245, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: timeout: Timed out receiving message from renderer: 299.997
  (Session info: headless chrome=111.0.5563.64)
Stacktrace:
0   chromedriver                        0x000000010c74c428 chromedriver + 4899880
1   chromedriver                        0x000000010c6c9a23 chromedriver + 4364835
2   chromedriver                        0x000000010c313bf6 chromedriver + 474102
3   chromedriver                        0x000000010c2fc4d9 chromedriver + 378073
4   chromedriver                        0x000000010c2fc08b chromedriver + 376971
5   chromedriver                        0x000000010c2fa8ba chromedriver + 370874
6   chromedriver                        0x000000010c2fadec chromedriver + 372204
7   chromedriver                        0x000000010c309b2c chromedriver + 432940
8   chromedriver                        0x000000010c30ae32 chromedriver + 437810
9   chromedriver                        0x000000010c31c794 chromedriver + 509844
10  chromedriver                        0x000000010c321dcb chromedriver + 531915
11  chromedriver                        0x000000010c2fb248 chromedriver + 373320
12  chromedriver                        0x000000010c31c5df chromedriver + 509407
13  chromedriver                        0x000000010c398da4 chromedriver + 1019300
14  chromedriver                        0x000000010c37d333 chromedriver + 906035
15  chromedriver                        0x000000010c34755f chromedriver + 685407
16  chromedriver                        0x000000010c348a7e chromedriver + 690814
17  chromedriver                        0x000000010c71979e chromedriver + 4691870
18  chromedriver                        0x000000010c71e961 chromedriver + 4712801
19  chromedriver                        0x000000010c7252ff chromedriver + 4739839
20  chromedriver                        0x000000010c71f85a chromedriver + 4716634
21  chromedriver                        0x000000010c6f1fce chromedriver + 4530126
22  chromedriver                        0x000000010c73f5c8 chromedriver + 4847048
23  chromedriver                        0x000000010c73f747 chromedriver + 4847431
24  chromedriver                        0x000000010c75487f chromedriver + 4933759
25  libsystem_pthread.dylib             0x00007ff80fe64259 _pthread_start + 125
26  libsystem_pthread.dylib             0x00007ff80fe5fc7b thread_start + 15

2023-03-13 04:25:06 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 434 items (at 1 items/min)
2023-03-13 04:25:06 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-13 04:25:06 [scrapy.extensions.feedexport] INFO: Stored json feed (434 items) in: company_info_0313.json
2023-03-13 04:25:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 576,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 80551,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 12955.922257,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 12, 20, 25, 6, 759563),
 'httpcompression/response_bytes': 258583,
 'httpcompression/response_count': 2,
 'item_scraped_count': 434,
 'log_count/ERROR': 1,
 'log_count/INFO': 224,
 'log_count/WARNING': 3,
 'memusage/max': 116203520,
 'memusage/startup': 106090496,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TimeoutException': 1,
 'start_time': datetime.datetime(2023, 3, 12, 16, 49, 10, 837306)}
2023-03-13 04:25:06 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-13 09:35:16 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-13 09:35:16 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-13 09:35:16 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-13 09:35:16 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-13 09:35:16 [scrapy.extensions.telnet] INFO: Telnet Password: 6a57b4d35c15302c
2023-03-13 09:35:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-13 09:35:16 [WDM] INFO: ====== WebDriver manager ======
2023-03-13 09:35:16 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-13 09:35:16 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-13 09:35:18 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-13 09:35:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-13 09:35:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-13 09:35:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-13 09:35:18 [scrapy.core.engine] INFO: Spider opened
2023-03-13 09:35:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-13 09:35:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2023-03-13 09:35:40 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:35:40 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:35:40 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:35:40 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:35:43 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:35:43 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:35:48 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:35:48 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:35:53 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:35:53 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:35:58 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:35:58 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:03 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:03 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:08 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:08 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:13 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:13 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-13 09:36:18 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:18 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:23 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:23 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:28 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:28 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:33 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:33 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:38 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:38 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-13 09:36:43 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method RefererMiddleware.request_scheduled of <scrapy.spidermiddlewares.referer.RefererMiddleware object at 0x11f8bba50>>
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/signal.py", line 38, in send_catch_log
    response = robustApply(
               ^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 368, in request_scheduled
    redirected_urls = request.meta.get("redirect_urls", [])
                      ^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'meta'
2023-03-13 09:36:43 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/commands/crawl.py", line 31, in run
    self.crawler_process.start()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/crawler.py", line 383, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1318, in run
    self.mainLoop()
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 1328, in mainLoop
    reactorBaseSelf.runUntilCurrent()
--- <exception caught here> ---
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/twisted/internet/base.py", line 994, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/reactor.py", line 54, in __call__
    return self._func(*self._a, **self._kw)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 169, in _next_request
    self.crawl(request)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 276, in crawl
    self._schedule_request(request, self.spider)
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 283, in _schedule_request
    if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/scheduler.py", line 241, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
builtins.AttributeError: 'generator' object has no attribute 'dont_filter'

2023-03-15 00:48:22 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-15 00:48:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-15 00:48:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-15 00:48:22 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-15 00:48:22 [scrapy.extensions.telnet] INFO: Telnet Password: 659222ef5601d9f4
2023-03-15 00:48:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-15 00:48:22 [WDM] INFO: ====== WebDriver manager ======
2023-03-15 00:48:22 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-15 00:48:23 [WDM] INFO: About to download new driver from https://chromedriver.storage.googleapis.com/111.0.5563.64/chromedriver_mac64.zip
2023-03-15 00:48:25 [WDM] INFO: Driver has been saved in cache [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563]
2023-03-15 00:48:27 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-15 00:48:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-15 00:48:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-15 00:48:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-15 00:48:27 [scrapy.core.engine] INFO: Spider opened
2023-03-15 00:48:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-15 00:48:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-15 00:49:14 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 158, in _next_request
    request = next(self.slot.start_requests)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 40, in start_requests
    self.max_page = int(self.wait.until(EC.visibility_of_element_located((By.ID,"pt9:t1::nb_pg55755"))))
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x0000000106d5d428 chromedriver + 4899880
1   chromedriver                        0x0000000106cdaa23 chromedriver + 4364835
2   chromedriver                        0x0000000106924bf6 chromedriver + 474102
3   chromedriver                        0x00000001069684f0 chromedriver + 750832
4   chromedriver                        0x0000000106968751 chromedriver + 751441
5   chromedriver                        0x00000001069ac834 chromedriver + 1030196
6   chromedriver                        0x000000010698e58d chromedriver + 906637
7   chromedriver                        0x00000001069a9b5b chromedriver + 1018715
8   chromedriver                        0x000000010698e333 chromedriver + 906035
9   chromedriver                        0x000000010695855f chromedriver + 685407
10  chromedriver                        0x0000000106959a7e chromedriver + 690814
11  chromedriver                        0x0000000106d2a79e chromedriver + 4691870
12  chromedriver                        0x0000000106d2f961 chromedriver + 4712801
13  chromedriver                        0x0000000106d362ff chromedriver + 4739839
14  chromedriver                        0x0000000106d3085a chromedriver + 4716634
15  chromedriver                        0x0000000106d02fce chromedriver + 4530126
16  chromedriver                        0x0000000106d505c8 chromedriver + 4847048
17  chromedriver                        0x0000000106d50747 chromedriver + 4847431
18  chromedriver                        0x0000000106d6587f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff80fdcd259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff80fdc8c7b thread_start + 15

2023-03-15 00:49:14 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-15 00:49:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 47.439294,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 14, 16, 49, 14, 755755),
 'log_count/ERROR': 1,
 'log_count/INFO': 14,
 'log_count/WARNING': 2,
 'memusage/max': 116699136,
 'memusage/startup': 116699136,
 'start_time': datetime.datetime(2023, 3, 14, 16, 48, 27, 316461)}
2023-03-15 00:49:14 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-15 00:57:03 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-15 00:57:03 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-15 00:57:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-15 00:57:03 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-15 00:57:03 [scrapy.extensions.telnet] INFO: Telnet Password: 931704b95e050131
2023-03-15 00:57:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-15 00:57:03 [WDM] INFO: ====== WebDriver manager ======
2023-03-15 00:57:03 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-15 00:57:04 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-15 00:57:06 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-15 00:57:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-15 00:57:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-15 00:57:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-15 00:57:06 [scrapy.core.engine] INFO: Spider opened
2023-03-15 00:57:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-15 00:57:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-15 00:57:56 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 158, in _next_request
    request = next(self.slot.start_requests)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 40, in start_requests
    self.max_page = int(self.wait.until(EC.visibility_of_element_located((By.ID,"pt9:t1::nb_pg55755"))))
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x0000000101f81428 chromedriver + 4899880
1   chromedriver                        0x0000000101efea23 chromedriver + 4364835
2   chromedriver                        0x0000000101b48bf6 chromedriver + 474102
3   chromedriver                        0x0000000101b8c4f0 chromedriver + 750832
4   chromedriver                        0x0000000101b8c751 chromedriver + 751441
5   chromedriver                        0x0000000101bd0834 chromedriver + 1030196
6   chromedriver                        0x0000000101bb258d chromedriver + 906637
7   chromedriver                        0x0000000101bcdb5b chromedriver + 1018715
8   chromedriver                        0x0000000101bb2333 chromedriver + 906035
9   chromedriver                        0x0000000101b7c55f chromedriver + 685407
10  chromedriver                        0x0000000101b7da7e chromedriver + 690814
11  chromedriver                        0x0000000101f4e79e chromedriver + 4691870
12  chromedriver                        0x0000000101f53961 chromedriver + 4712801
13  chromedriver                        0x0000000101f5a2ff chromedriver + 4739839
14  chromedriver                        0x0000000101f5485a chromedriver + 4716634
15  chromedriver                        0x0000000101f26fce chromedriver + 4530126
16  chromedriver                        0x0000000101f745c8 chromedriver + 4847048
17  chromedriver                        0x0000000101f74747 chromedriver + 4847431
18  chromedriver                        0x0000000101f8987f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff80fdcd259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff80fdc8c7b thread_start + 15

2023-03-15 00:57:56 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-15 00:57:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 49.351803,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 14, 16, 57, 56, 67026),
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'log_count/WARNING': 2,
 'memusage/max': 109752320,
 'memusage/startup': 109752320,
 'start_time': datetime.datetime(2023, 3, 14, 16, 57, 6, 715223)}
2023-03-15 00:57:56 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-15 00:58:53 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-15 00:58:53 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-15 00:58:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-15 00:58:53 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-15 00:58:53 [scrapy.extensions.telnet] INFO: Telnet Password: 6f064a16d5257979
2023-03-15 00:58:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-15 00:58:53 [WDM] INFO: ====== WebDriver manager ======
2023-03-15 00:58:54 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-15 00:58:54 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-15 00:58:55 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-15 00:58:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-15 00:58:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-15 00:58:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-15 00:58:55 [scrapy.core.engine] INFO: Spider opened
2023-03-15 00:58:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-15 00:58:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-15 01:00:13 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 158, in _next_request
    request = next(self.slot.start_requests)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 40, in start_requests
    self.max_page = int(self.wait.until(EC.visibility_of_element_located((By.ID,"pt9:t1::nb_pg55755"))))
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x0000000101d04428 chromedriver + 4899880
1   chromedriver                        0x0000000101c81a23 chromedriver + 4364835
2   chromedriver                        0x00000001018cbbf6 chromedriver + 474102
3   chromedriver                        0x000000010190f4f0 chromedriver + 750832
4   chromedriver                        0x000000010190f751 chromedriver + 751441
5   chromedriver                        0x0000000101953834 chromedriver + 1030196
6   chromedriver                        0x000000010193558d chromedriver + 906637
7   chromedriver                        0x0000000101950b5b chromedriver + 1018715
8   chromedriver                        0x0000000101935333 chromedriver + 906035
9   chromedriver                        0x00000001018ff55f chromedriver + 685407
10  chromedriver                        0x0000000101900a7e chromedriver + 690814
11  chromedriver                        0x0000000101cd179e chromedriver + 4691870
12  chromedriver                        0x0000000101cd6961 chromedriver + 4712801
13  chromedriver                        0x0000000101cdd2ff chromedriver + 4739839
14  chromedriver                        0x0000000101cd785a chromedriver + 4716634
15  chromedriver                        0x0000000101ca9fce chromedriver + 4530126
16  chromedriver                        0x0000000101cf75c8 chromedriver + 4847048
17  chromedriver                        0x0000000101cf7747 chromedriver + 4847431
18  chromedriver                        0x0000000101d0c87f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff80fdcd259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff80fdc8c7b thread_start + 15

2023-03-15 01:00:13 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-15 01:00:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 78.248127,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 14, 17, 0, 13, 893891),
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'log_count/WARNING': 2,
 'memusage/max': 108105728,
 'memusage/startup': 108105728,
 'start_time': datetime.datetime(2023, 3, 14, 16, 58, 55, 645764)}
2023-03-15 01:00:13 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-15 01:04:18 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-15 01:04:18 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-15 01:04:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-15 01:04:18 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-15 01:04:18 [scrapy.extensions.telnet] INFO: Telnet Password: f77e2d6a5e2242e4
2023-03-15 01:04:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-15 01:04:18 [WDM] INFO: ====== WebDriver manager ======
2023-03-15 01:04:18 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-15 01:04:18 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-15 01:04:20 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-15 01:04:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-15 01:04:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-15 01:04:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-15 01:04:20 [scrapy.core.engine] INFO: Spider opened
2023-03-15 01:04:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-15 01:04:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-15 01:04:36 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/engine.py", line 158, in _next_request
    request = next(self.slot.start_requests)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 40, in start_requests
    self.max_page = int(self.wait.until(EC.visibility_of_element_located((By.ID,"pt9:t1::nb_pg56265")))) #需修改，因id會變
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'WebElement'
2023-03-15 01:04:36 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-15 01:04:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 16.239592,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 14, 17, 4, 36, 450782),
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'log_count/WARNING': 2,
 'memusage/max': 108556288,
 'memusage/startup': 108556288,
 'start_time': datetime.datetime(2023, 3, 14, 17, 4, 20, 211190)}
2023-03-15 01:04:36 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-15 01:11:19 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: Spider_POC_SSC)
2023-03-15 01:11:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.11.2 (main, Feb 16 2023, 03:07:35) [Clang 14.0.0 (clang-1400.0.29.202)], pyOpenSSL 23.0.0 (OpenSSL 3.0.8 7 Feb 2023), cryptography 39.0.2, Platform macOS-13.1-x86_64-i386-64bit
2023-03-15 01:11:19 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Spider_POC_SSC',
 'LOG_FILE': 'spider_POC_SSC_log.ini',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'Spider_POC_SSC.spiders',
 'RETRY_HTTP_CODES': [500, 502, 503, 504, 408],
 'RETRY_TIMES': 3,
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Spider_POC_SSC.spiders']}
2023-03-15 01:11:19 [py.warnings] WARNING: /Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

2023-03-15 01:11:19 [scrapy.extensions.telnet] INFO: Telnet Password: 0b9178751b5aeb9f
2023-03-15 01:11:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2023-03-15 01:11:19 [WDM] INFO: ====== WebDriver manager ======
2023-03-15 01:11:20 [WDM] INFO: Get LATEST chromedriver version for google-chrome 111.0.5563
2023-03-15 01:11:20 [WDM] INFO: Driver [/Users/littlep/.wdm/drivers/chromedriver/mac64/111.0.5563/chromedriver] found in cache
2023-03-15 01:11:21 [scrapy.middleware] WARNING: Disabled SeleniumMiddleware: SELENIUM_DRIVER_NAME and SELENIUM_DRIVER_EXECUTABLE_PATH must be set
2023-03-15 01:11:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'Spider_POC_SSC.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-03-15 01:11:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-03-15 01:11:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2023-03-15 01:11:21 [scrapy.core.engine] INFO: Spider opened
2023-03-15 01:11:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-03-15 01:11:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-03-15 01:12:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 2 items (at 2 items/min)
2023-03-15 01:13:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 4 items (at 2 items/min)
2023-03-15 01:14:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 6 items (at 2 items/min)
2023-03-15 01:15:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 8 items (at 2 items/min)
2023-03-15 01:16:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 10 items (at 2 items/min)
2023-03-15 01:17:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 12 items (at 2 items/min)
2023-03-15 01:18:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 14 items (at 2 items/min)
2023-03-15 01:19:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 16 items (at 2 items/min)
2023-03-15 01:20:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 18 items (at 2 items/min)
2023-03-15 01:21:41 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 20 items (at 2 items/min)
2023-03-15 01:22:45 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 22 items (at 2 items/min)
2023-03-15 01:23:53 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 24 items (at 2 items/min)
2023-03-15 01:24:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 25 items (at 1 items/min)
2023-03-15 01:25:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 27 items (at 2 items/min)
2023-03-15 01:26:39 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 29 items (at 2 items/min)
2023-03-15 01:27:42 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 31 items (at 2 items/min)
2023-03-15 01:28:45 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 33 items (at 2 items/min)
2023-03-15 01:29:49 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 35 items (at 2 items/min)
2023-03-15 01:30:54 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 37 items (at 2 items/min)
2023-03-15 01:31:26 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 38 items (at 1 items/min)
2023-03-15 01:32:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 40 items (at 2 items/min)
2023-03-15 01:33:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 42 items (at 2 items/min)
2023-03-15 01:34:40 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 44 items (at 2 items/min)
2023-03-15 01:35:47 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 46 items (at 2 items/min)
2023-03-15 01:36:50 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 48 items (at 2 items/min)
2023-03-15 01:37:22 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 49 items (at 1 items/min)
2023-03-15 01:38:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 51 items (at 2 items/min)
2023-03-15 01:39:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 53 items (at 2 items/min)
2023-03-15 01:40:31 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 55 items (at 2 items/min)
2023-03-15 01:41:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 57 items (at 2 items/min)
2023-03-15 01:42:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 59 items (at 2 items/min)
2023-03-15 01:43:39 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 61 items (at 2 items/min)
2023-03-15 01:44:40 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 63 items (at 2 items/min)
2023-03-15 01:45:47 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 65 items (at 2 items/min)
2023-03-15 01:46:50 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 67 items (at 2 items/min)
2023-03-15 01:47:52 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 69 items (at 2 items/min)
2023-03-15 01:48:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 70 items (at 1 items/min)
2023-03-15 01:49:32 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 72 items (at 2 items/min)
2023-03-15 01:50:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 74 items (at 2 items/min)
2023-03-15 01:51:40 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 76 items (at 2 items/min)
2023-03-15 01:52:44 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 78 items (at 2 items/min)
2023-03-15 01:53:47 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 80 items (at 2 items/min)
2023-03-15 01:54:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 81 items (at 1 items/min)
2023-03-15 01:55:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 83 items (at 2 items/min)
2023-03-15 01:56:45 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 85 items (at 2 items/min)
2023-03-15 01:57:50 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 87 items (at 2 items/min)
2023-03-15 01:58:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 88 items (at 1 items/min)
2023-03-15 01:59:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 90 items (at 2 items/min)
2023-03-15 02:00:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 92 items (at 2 items/min)
2023-03-15 02:01:36 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 94 items (at 2 items/min)
2023-03-15 02:02:47 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 96 items (at 2 items/min)
2023-03-15 02:03:51 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 98 items (at 2 items/min)
2023-03-15 02:04:23 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 99 items (at 1 items/min)
2023-03-15 02:06:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com.tw> (referer: None)
Traceback (most recent call last):
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/defer.py", line 257, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/utils/python.py", line 312, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/referer.py", line 353, in <genexpr>
    return (self._set_referer(r, response) for r in result or ())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/urllength.py", line 27, in <genexpr>
    return (r for r in result or () if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 31, in <genexpr>
    return (r for r in result or () if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 104, in process_sync
    for r in iterable:
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 74, in get_company_finance_data
    company_name, company_MDN = self.get_company_name_MDN()
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/littlep/Desktop/數據分析/spider_poc/Spider_POC/Spider_POC_SSC/Spider_POC_SSC/spiders/ssc_company_finance.py", line 103, in get_company_name_MDN
    self.wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id="pt2:t2:0:c3"]/span/span')))
  File "/Users/littlep/.local/share/virtualenvs/spider_poc-XkJMsOn7/lib/python3.11/site-packages/selenium/webdriver/support/wait.py", line 95, in until
    raise TimeoutException(message, screen, stacktrace)
selenium.common.exceptions.TimeoutException: Message: 
Stacktrace:
0   chromedriver                        0x00000001068e5428 chromedriver + 4899880
1   chromedriver                        0x0000000106862a23 chromedriver + 4364835
2   chromedriver                        0x00000001064acbf6 chromedriver + 474102
3   chromedriver                        0x00000001064f04f0 chromedriver + 750832
4   chromedriver                        0x00000001064f0751 chromedriver + 751441
5   chromedriver                        0x0000000106534834 chromedriver + 1030196
6   chromedriver                        0x000000010651658d chromedriver + 906637
7   chromedriver                        0x0000000106531b5b chromedriver + 1018715
8   chromedriver                        0x0000000106516333 chromedriver + 906035
9   chromedriver                        0x00000001064e055f chromedriver + 685407
10  chromedriver                        0x00000001064e1a7e chromedriver + 690814
11  chromedriver                        0x00000001068b279e chromedriver + 4691870
12  chromedriver                        0x00000001068b7961 chromedriver + 4712801
13  chromedriver                        0x00000001068be2ff chromedriver + 4739839
14  chromedriver                        0x00000001068b885a chromedriver + 4716634
15  chromedriver                        0x000000010688afce chromedriver + 4530126
16  chromedriver                        0x00000001068d85c8 chromedriver + 4847048
17  chromedriver                        0x00000001068d8747 chromedriver + 4847431
18  chromedriver                        0x00000001068ed87f chromedriver + 4933759
19  libsystem_pthread.dylib             0x00007ff80fdcd259 _pthread_start + 125
20  libsystem_pthread.dylib             0x00007ff80fdc8c7b thread_start + 15

2023-03-15 02:06:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 100 items (at 1 items/min)
2023-03-15 02:06:15 [scrapy.core.engine] INFO: Closing spider (finished)
2023-03-15 02:06:15 [scrapy.extensions.feedexport] INFO: Stored json feed (100 items) in: comapny_finance_0315.json
2023-03-15 02:06:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 646,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 49849,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 3293.63138,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 3, 14, 18, 6, 15, 533763),
 'httpcompression/response_bytes': 151508,
 'httpcompression/response_count': 2,
 'item_scraped_count': 100,
 'log_count/ERROR': 1,
 'log_count/INFO': 68,
 'log_count/WARNING': 2,
 'memusage/max': 125399040,
 'memusage/startup': 105205760,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TimeoutException': 1,
 'start_time': datetime.datetime(2023, 3, 14, 17, 11, 21, 902383)}
2023-03-15 02:06:15 [scrapy.core.engine] INFO: Spider closed (finished)
